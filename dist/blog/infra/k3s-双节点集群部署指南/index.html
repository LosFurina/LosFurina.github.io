<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="icon" href="/favicon.ico"><!-- Theme Initialization Script --><script>
	const theme = (() => {
		if (
			typeof localStorage !== "undefined" &&
			localStorage.getItem("theme")
		) {
			return localStorage.getItem("theme");
		}
		if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
			return "dark";
		}
		return "light";
	})();

	if (theme === "dark") {
		document.documentElement.setAttribute("data-theme", "dark");
		document.documentElement.classList.add("dark");
	} else {
		document.documentElement.setAttribute("data-theme", "light");
		document.documentElement.classList.remove("dark");
	}

	window.localStorage.setItem("theme", theme);
</script><link rel="sitemap" href="/sitemap-index.xml"><link rel="alternate" type="application/rss+xml" title="Wayne" href="https://blog.liweijun.com/rss.xml"><meta name="generator" content="Astro v5.17.1"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://blog.liweijun.com/blog/infra/k3s-%E5%8F%8C%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/"><!-- Primary Meta Tags --><title>K3s 双节点集群部署指南</title><meta name="title" content="K3s 双节点集群部署指南"><meta name="description" content="网络连通性：两台机器之间需要互通。如果是跨云厂商，建议使用 Tailscale 或 WireGuard 建立虚拟内网，或者确保防火墙开放了..."><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://blog.liweijun.com/blog/infra/k3s-%E5%8F%8C%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/"><meta property="og:title" content="K3s 双节点集群部署指南"><meta property="og:description" content="网络连通性：两台机器之间需要互通。如果是跨云厂商，建议使用 Tailscale 或 WireGuard 建立虚拟内网，或者确保防火墙开放了..."><meta property="og:image" content="https://blog.liweijun.com/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://blog.liweijun.com/blog/infra/k3s-%E5%8F%8C%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/"><meta property="twitter:title" content="K3s 双节点集群部署指南"><meta property="twitter:description" content="网络连通性：两台机器之间需要互通。如果是跨云厂商，建议使用 Tailscale 或 WireGuard 建立虚拟内网，或者确保防火墙开放了..."><meta property="twitter:image" content="https://blog.liweijun.com/_astro/blog-placeholder-1.Bx0Zcyzv.jpg"><!-- Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","name":"Wayne","description":"DevOps and infrastructure engineering notes, projects, and practical automation experiments by Wayne Li.","url":"https://blog.liweijun.com/","inLanguage":"en"}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Person","name":"Wayne Li","url":"https://blog.liweijun.com/","email":"mailto:me@liweijun.com","sameAs":["https://github.com/LosFurina","https://x.com/MicroNahida"],"jobTitle":"DevOps / Infrastructure Engineer"}</script><link rel="stylesheet" href="/_astro/index.DCFK0IPC.css">
<link rel="stylesheet" href="/_astro/about.DVvCJqHw.css">
<style>a[data-astro-cid-eimmu3lg]{display:inline-flex;align-items:center;gap:.5rem;padding:.4rem .75rem;text-decoration:none;font-size:.9rem;font-weight:500;color:var(--header-text, rgb(var(--gray-dark)));border-radius:999px;transition:color .2s ease,background .2s ease;position:relative}a[data-astro-cid-eimmu3lg]:hover{background:var(--header-hover, rgba(179, 106, 74, .08));color:var(--header-text-strong, rgb(var(--gray-dark)))}a[data-astro-cid-eimmu3lg]:focus-visible{outline:2px solid var(--header-accent, #b36a4a);outline-offset:2px}a[data-astro-cid-eimmu3lg].active{color:var(--header-accent, #b36a4a)}a[data-astro-cid-eimmu3lg].active:after{content:"";position:absolute;left:50%;bottom:4px;width:4px;height:4px;margin-left:-2px;border-radius:999px;background:var(--header-accent, #b36a4a)}.nav-icon[data-astro-cid-eimmu3lg]{display:inline-flex;width:16px;height:16px}.nav-icon[data-astro-cid-eimmu3lg] svg{width:16px;height:16px;fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round}@media(max-width:720px){a[data-astro-cid-eimmu3lg]{width:100%;justify-content:flex-start;padding:.6rem .9rem}}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <div class="nav-left" data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>Wayne</a></h2> </div> <div class="internal-links" id="mobileMenu" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> <span class="nav-icon" aria-hidden="true" data-astro-cid-eimmu3lg> <svg viewBox="0 0 24 24" fill="none" stroke-width="1.6" data-astro-cid-3ef6ksr2> <path d="M3 11.5L12 4l9 7.5" data-astro-cid-3ef6ksr2></path> <path d="M6.5 10.5V20h11V10.5" data-astro-cid-3ef6ksr2></path> </svg> </span> <span class="nav-text" data-astro-cid-eimmu3lg> 
Home
</span> </a>  <a href="/blog" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> <span class="nav-icon" aria-hidden="true" data-astro-cid-eimmu3lg> <svg viewBox="0 0 24 24" fill="none" stroke-width="1.6" data-astro-cid-3ef6ksr2> <rect x="6" y="4.5" width="12" height="15" rx="2" data-astro-cid-3ef6ksr2></rect> <path d="M9 9h6" data-astro-cid-3ef6ksr2></path> <path d="M9 13h6" data-astro-cid-3ef6ksr2></path> </svg> </span> <span class="nav-text" data-astro-cid-eimmu3lg> 
Blog
</span> </a>  <a href="/projects" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> <span class="nav-icon" aria-hidden="true" data-astro-cid-eimmu3lg> <svg viewBox="0 0 24 24" fill="none" stroke-width="1.6" data-astro-cid-3ef6ksr2> <rect x="4" y="4" width="7" height="7" rx="1.5" data-astro-cid-3ef6ksr2></rect> <rect x="13" y="4" width="7" height="7" rx="1.5" data-astro-cid-3ef6ksr2></rect> <rect x="4" y="13" width="7" height="7" rx="1.5" data-astro-cid-3ef6ksr2></rect> <rect x="13" y="13" width="7" height="7" rx="1.5" data-astro-cid-3ef6ksr2></rect> </svg> </span> <span class="nav-text" data-astro-cid-eimmu3lg> 
Projects
</span> </a>  <a href="/resume" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> <span class="nav-icon" aria-hidden="true" data-astro-cid-eimmu3lg> <svg viewBox="0 0 24 24" fill="none" stroke-width="1.6" data-astro-cid-3ef6ksr2> <path d="M7 3.5h7l4 4v13a2 2 0 0 1-2 2H7a2 2 0 0 1-2-2V5.5a2 2 0 0 1 2-2z" data-astro-cid-3ef6ksr2></path> <path d="M14 3.5V8h4" data-astro-cid-3ef6ksr2></path> <path d="M9 12h6" data-astro-cid-3ef6ksr2></path> <path d="M9 16h6" data-astro-cid-3ef6ksr2></path> </svg> </span> <span class="nav-text" data-astro-cid-eimmu3lg> 
Resume
</span> </a>  <a href="/contact" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> <span class="nav-icon" aria-hidden="true" data-astro-cid-eimmu3lg> <svg viewBox="0 0 24 24" fill="none" stroke-width="1.6" data-astro-cid-3ef6ksr2> <path d="M4 6.5h16a2 2 0 0 1 2 2v9a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2v-9a2 2 0 0 1 2-2z" data-astro-cid-3ef6ksr2></path> <path d="M4 8l8 6 8-6" data-astro-cid-3ef6ksr2></path> </svg> </span> <span class="nav-text" data-astro-cid-eimmu3lg> 
Contact
</span> </a>  <!-- Tools Dropdown --> <div class="dropdown" id="toolsDropdown" data-astro-cid-3ef6ksr2> <button class="dropdown-trigger" id="toolsBtn" data-astro-cid-3ef6ksr2>
More
<svg class="dropdown-arrow" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" width="12" height="12" data-astro-cid-3ef6ksr2> <path stroke-linecap="round" stroke-linejoin="round" d="m19.5 8.25-7.5 7.5-7.5-7.5" data-astro-cid-3ef6ksr2></path> </svg> </button> <div class="dropdown-menu" id="toolsMenu" data-astro-cid-3ef6ksr2> <!-- 本站页面 --> <a href="/about" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>👤</span>
About
</a> <a href="/uses" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>🛠️</span>
Uses
</a> <a href="/now" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>⏰</span>
Now
</a> <a href="/bookmarks" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>🔖</span>
Bookmarks
</a> <a href="/guestbook" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>📝</span>
Guestbook
</a> <div class="dropdown-divider" data-astro-cid-3ef6ksr2></div> <a href="/keys" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>🔐</span>
GPG Keys
</a> <div class="dropdown-divider" data-astro-cid-3ef6ksr2></div> <!-- 外部链接 --> <a href="https://github.com/LosFurina" target="_blank" rel="noopener" class="dropdown-item" data-astro-cid-3ef6ksr2> <span class="item-icon" data-astro-cid-3ef6ksr2>📦</span>
GitHub
<span class="external-icon" data-astro-cid-3ef6ksr2>↗</span> </a> </div> </div> </div> <div class="nav-right" data-astro-cid-3ef6ksr2> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://github.com/LosFurina" target="_blank" title="GitHub" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>GitHub</span> <svg viewBox="0 0 16 16" aria-hidden="true" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path> </svg> </a> <a href="https://x.com/MicroNahida" target="_blank" rel="noopener" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>X (Twitter)</span> <svg viewBox="0 0 24 24" aria-hidden="true" data-astro-cid-3ef6ksr2> <path fill="currentColor" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" data-astro-cid-3ef6ksr2></path> </svg> </a> </div> <button id="theme-toggle" aria-label="Toggle theme" data-astro-cid-x3pjskd3> <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-x3pjskd3> <circle cx="12" cy="12" r="5" data-astro-cid-x3pjskd3></circle> <line x1="12" y1="1" x2="12" y2="3" data-astro-cid-x3pjskd3></line> <line x1="12" y1="21" x2="12" y2="23" data-astro-cid-x3pjskd3></line> <line x1="4.22" y1="4.22" x2="5.64" y2="5.64" data-astro-cid-x3pjskd3></line> <line x1="18.36" y1="18.36" x2="19.78" y2="19.78" data-astro-cid-x3pjskd3></line> <line x1="1" y1="12" x2="3" y2="12" data-astro-cid-x3pjskd3></line> <line x1="21" y1="12" x2="23" y2="12" data-astro-cid-x3pjskd3></line> <line x1="4.22" y1="19.78" x2="5.64" y2="18.36" data-astro-cid-x3pjskd3></line> <line x1="18.36" y1="5.64" x2="19.78" y2="4.22" data-astro-cid-x3pjskd3></line> </svg> <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" data-astro-cid-x3pjskd3> <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z" data-astro-cid-x3pjskd3></path> </svg> </button>  <script type="module">const t=()=>{const e=document.documentElement;e.getAttribute("data-theme")==="dark"?(e.setAttribute("data-theme","light"),e.classList.remove("dark"),localStorage.setItem("theme","light")):(e.setAttribute("data-theme","dark"),e.classList.add("dark"),localStorage.setItem("theme","dark"))};document.getElementById("theme-toggle")?.addEventListener("click",t);</script> <!-- Hamburger Menu Button (Mobile Only) --> <button class="hamburger" id="hamburgerBtn" aria-label="Toggle menu" data-astro-cid-3ef6ksr2> <span class="hamburger-line" data-astro-cid-3ef6ksr2></span> <span class="hamburger-line" data-astro-cid-3ef6ksr2></span> <span class="hamburger-line" data-astro-cid-3ef6ksr2></span> </button> </div> </nav> </header>  <script type="module">const n=document.querySelector("header");function c(){window.scrollY>10?n?.classList.add("scrolled"):n?.classList.remove("scrolled")}window.addEventListener("scroll",c,{passive:!0});c();const t=document.getElementById("hamburgerBtn"),o=document.getElementById("mobileMenu");t?.addEventListener("click",e=>{e.stopPropagation(),t.classList.toggle("open"),o?.classList.toggle("open")});o?.querySelectorAll("a").forEach(e=>{e.addEventListener("click",()=>{t?.classList.remove("open"),o?.classList.remove("open")})});const s=document.getElementById("toolsDropdown"),l=document.getElementById("toolsBtn");l?.addEventListener("click",e=>{e.stopPropagation(),s?.classList.toggle("open")});document.addEventListener("click",e=>{s?.contains(e.target)||s?.classList.remove("open"),!o?.contains(e.target)&&!t?.contains(e.target)&&(t?.classList.remove("open"),o?.classList.remove("open"))});document.addEventListener("keydown",e=>{e.key==="Escape"&&(s?.classList.remove("open"),t?.classList.remove("open"),o?.classList.remove("open"))});</script> <div class="blog-container" data-astro-cid-bvzihdzo> <aside class="toc-sidebar" data-astro-cid-bvzihdzo> <div class="toc-wrapper" data-astro-cid-bvzihdzo> <nav class="toc" aria-label="Table of Contents" data-astro-cid-xvrfupwn><h4 class="toc-title" data-astro-cid-xvrfupwn><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" width="16" height="16" data-astro-cid-xvrfupwn><path stroke-linecap="round" stroke-linejoin="round" d="M8.25 6.75h12M8.25 12h12m-12 5.25h12M3.75 6.75h.007v.008H3.75V6.75Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0ZM3.75 12h.007v.008H3.75V12Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Zm-.375 5.25h.007v.008H3.75v-.008Zm.375 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Z" data-astro-cid-xvrfupwn></path></svg>
On this page
</h4><ul class="toc-list" data-astro-cid-xvrfupwn><li class="toc-item depth-2" data-astro-cid-xvrfupwn><a href="#背景与目标" class="toc-link" data-astro-cid-xvrfupwn>背景与目标</a></li><li class="toc-item depth-2" data-astro-cid-xvrfupwn><a href="#核心内容" class="toc-link" data-astro-cid-xvrfupwn>核心内容</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-1-我准备在两台vps上部署k3s然后作为两个界点串联起来" class="toc-link" data-astro-cid-xvrfupwn>主题 1. 我准备在两台vps上部署k3s，然后作为两个界点串联起来</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-2-我已经有一个node在运行了但是我不想让这个节点作为主要节点因为这台" class="toc-link" data-astro-cid-xvrfupwn>主题 2. 我已经有一个node在运行了，但是我不想让这个节点作为主要节点，因为这台…</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案一保留旧节点为-server但禁止它跑业务-pod" class="toc-link" data-astro-cid-xvrfupwn>方案一：保留旧节点为 Server，但禁止它跑业务 Pod</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案二性能强的作为-server旧的作为-agent-推荐" class="toc-link" data-astro-cid-xvrfupwn>方案二：性能强的作为 Server，旧的作为 Agent (推荐)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对你的-revieu-项目架构建议" class="toc-link" data-astro-cid-xvrfupwn>针对你的 RevieU 项目架构建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#一个进阶的小技巧" class="toc-link" data-astro-cid-xvrfupwn>一个进阶的小技巧</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-3-server和agent有什么区别" class="toc-link" data-astro-cid-xvrfupwn>主题 3. server和agent有什么区别</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-server-节点-control-plane" class="toc-link" data-astro-cid-xvrfupwn>1. Server 节点 (Control Plane)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-agent-节点-worker-node" class="toc-link" data-astro-cid-xvrfupwn>2. Agent 节点 (Worker Node)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#核心对比表" class="toc-link" data-astro-cid-xvrfupwn>核心对比表</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#结合你的情况为什么你要在意这个区别" class="toc-link" data-astro-cid-xvrfupwn>结合你的情况：为什么你要在意这个区别。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-4-那我还是应该先在新服务器上安装k3s然后打通连哥哥节点" class="toc-link" data-astro-cid-xvrfupwn>主题 4. 那我还是应该先在新服务器上安装k3s然后打通连哥哥节点</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#第一步清理旧节点如果已安装-k3s" class="toc-link" data-astro-cid-xvrfupwn>第一步：清理旧节点（如果已安装 k3s）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#第二步在新服务器上部署-server" class="toc-link" data-astro-cid-xvrfupwn>第二步：在新服务器上部署 Server</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#第三步将旧服务器作为-agent-加入" class="toc-link" data-astro-cid-xvrfupwn>第三步：将旧服务器作为 Agent 加入</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#第四步打通网络核心关键" class="toc-link" data-astro-cid-xvrfupwn>第四步：打通网络（核心关键）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对你的配置建议" class="toc-link" data-astro-cid-xvrfupwn>针对你的配置建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-5-我的老服务器上的东西我不想删掉啊" class="toc-link" data-astro-cid-xvrfupwn>主题 5. 我的老服务器上的东西我不想删掉啊</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#最稳妥的方案双系统并行" class="toc-link" data-astro-cid-xvrfupwn>最稳妥的方案：双系统并行</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#️-必须要解决的冲突端口限制" class="toc-link" data-astro-cid-xvrfupwn>️ 必须要解决的冲突：端口限制</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#保护老服务器的生存空间" class="toc-link" data-astro-cid-xvrfupwn>保护老服务器的“生存空间”</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-6-如果在运行服务的node上添加污点pod会自动迁移到新服务器上吗" class="toc-link" data-astro-cid-xvrfupwn>主题 6. 如果在运行服务的node上添加污点，pod会自动迁移到新服务器上吗</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-现状已经运行的-pod" class="toc-link" data-astro-cid-xvrfupwn>1. 现状：已经运行的 Pod</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-如何让它们自动迁移" class="toc-link" data-astro-cid-xvrfupwn>2. 如何让它们“自动”迁移。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-针对你的实际操作建议" class="toc-link" data-astro-cid-xvrfupwn>3. 针对你的实际操作建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#总结" class="toc-link" data-astro-cid-xvrfupwn>总结</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-7-我用内网10001还需要https吗" class="toc-link" data-astro-cid-xvrfupwn>主题 7. 我用内网，10.0.0.1,还需要https吗</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-正确的内网安装命令" class="toc-link" data-astro-cid-xvrfupwn>1. 正确的内网安装命令</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-为什么内网也强制-https" class="toc-link" data-astro-cid-xvrfupwn>2. 为什么内网也强制 HTTPS。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-️-关键检查server-端是否监听了内网-ip" class="toc-link" data-astro-cid-xvrfupwn>3. ️ 关键检查：Server 端是否监听了内网 IP。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-针对你两台-vps-的网络排烟troubleshooting" class="toc-link" data-astro-cid-xvrfupwn>4. 针对你两台 VPS 的网络排烟（Troubleshooting）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#一个关于-arch-linux-的小提醒" class="toc-link" data-astro-cid-xvrfupwn>一个关于 Arch Linux 的小提醒</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-8-老服务器上必须开放的端口有哪些" class="toc-link" data-astro-cid-xvrfupwn>主题 8. 老服务器上必须开放的端口有哪些</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-k3s-内部通讯必须开放的端口" class="toc-link" data-astro-cid-xvrfupwn>1. K3s 内部通讯必须开放的端口</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-业务访问端口容易冲突的地方" class="toc-link" data-astro-cid-xvrfupwn>2. 业务访问端口（容易冲突的地方）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-如何在老服务器上温和地安装" class="toc-link" data-astro-cid-xvrfupwn>3. 如何在老服务器上“温和”地安装。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-总结你的防火墙检查清单" class="toc-link" data-astro-cid-xvrfupwn>4. 总结：你的防火墙检查清单</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#避坑指南" class="toc-link" data-astro-cid-xvrfupwn>避坑指南</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-9-这个可以用ufw实现吗还是怎么做" class="toc-link" data-astro-cid-xvrfupwn>主题 9. 这个可以用ufw实现吗，还是怎么做</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-使用-ufw-授权内网-ip" class="toc-link" data-astro-cid-xvrfupwn>1. 使用 ufw 授权内网 IP</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-检查并启用-ufw" class="toc-link" data-astro-cid-xvrfupwn>2. 检查并启用 ufw</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-更进阶的做法基于网段授权" class="toc-link" data-astro-cid-xvrfupwn>3. 更进阶的做法：基于网段授权</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#️-一个必须要提醒你的arch-用户细节" class="toc-link" data-astro-cid-xvrfupwn>️ 一个必须要提醒你的“Arch 用户”细节</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#总结操作清单" class="toc-link" data-astro-cid-xvrfupwn>总结操作清单</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-10-老服务器ip是10001新服务器是10004" class="toc-link" data-astro-cid-xvrfupwn>主题 10. 老服务器ip是10.0.0.1,新服务器是10.0.0.4</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-在老服务器-10001-上操作" class="toc-link" data-astro-cid-xvrfupwn>1. 在老服务器 (10.0.0.1) 上操作</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-在新服务器-10004-上操作" class="toc-link" data-astro-cid-xvrfupwn>2. 在新服务器 (10.0.0.4) 上操作</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-部署串联命令" class="toc-link" data-astro-cid-xvrfupwn>3. 部署串联命令</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对-revieu-项目的小提示" class="toc-link" data-astro-cid-xvrfupwn>针对 RevieU 项目的小提示</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-1-这是什么意思我是ubuntu" class="toc-link" data-astro-cid-xvrfupwn>排查场景 1. 这是什么意思，我是ubuntu</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#为什么要这么做" class="toc-link" data-astro-cid-xvrfupwn>为什么要这么做。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#具体操作步骤" class="toc-link" data-astro-cid-xvrfupwn>具体操作步骤</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#这样安全吗" class="toc-link" data-astro-cid-xvrfupwn>这样安全吗。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#给你的-revieu-项目避个坑" class="toc-link" data-astro-cid-xvrfupwn>给你的 RevieU 项目避个坑</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-11-你这一通操作是把我原来的服务器变成了agent新服务器变成了serv" class="toc-link" data-astro-cid-xvrfupwn>主题 11. 你这一通操作，是把我原来的服务器变成了agent，新服务器变成了serv…</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-保护老服务器的现有服务" class="toc-link" data-astro-cid-xvrfupwn>1. 保护老服务器的现有服务</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-让-revieu-项目跑得更稳" class="toc-link" data-astro-cid-xvrfupwn>2. 让 RevieU 项目跑得更稳</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-现在的架构长这样" class="toc-link" data-astro-cid-xvrfupwn>3. 现在的架构长这样</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#如果你一定要反过来老服务器当-server会发生什么" class="toc-link" data-astro-cid-xvrfupwn>如果你一定要反过来（老服务器当 Server），会发生什么。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#下一步操作建议" class="toc-link" data-astro-cid-xvrfupwn>下一步操作建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-12-在解释一下这些参数" class="toc-link" data-astro-cid-xvrfupwn>主题 12. 在解释一下这些参数</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#参数详细拆解" class="toc-link" data-astro-cid-xvrfupwn>参数详细拆解</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#整体架构逻辑" class="toc-link" data-astro-cid-xvrfupwn>整体架构逻辑</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#接下来该做什么" class="toc-link" data-astro-cid-xvrfupwn>接下来该做什么。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-13-你设置了nodeip是一个vpn-ip那我使用ingress的时候还" class="toc-link" data-astro-cid-xvrfupwn>主题 13. 你设置了nodeip是一个vpn ip，那我使用ingress的时候，还…</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-为什么-ingress-依然正常工作" class="toc-link" data-astro-cid-xvrfupwn>1. 为什么 Ingress 依然正常工作。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-流量的具体路径" class="toc-link" data-astro-cid-xvrfupwn>2. 流量的具体路径</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-需要注意的一个小坑重点" class="toc-link" data-astro-cid-xvrfupwn>3. 需要注意的一个小坑（重点）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-进阶如何彻底利用你的双节点" class="toc-link" data-astro-cid-xvrfupwn>4. 进阶：如何彻底利用你的双节点。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#验证建议" class="toc-link" data-astro-cid-xvrfupwn>验证建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-2-nc--zv-10001-6443-29s-rootcc-11" class="toc-link" data-astro-cid-xvrfupwn>排查场景 2. nc -zv 10.0.0.1 6443 29s root@cc 11:…</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-查看具体的报错原因" class="toc-link" data-astro-cid-xvrfupwn>1. 查看具体的报错原因</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-彻底清理环境最推荐的快速修复" class="toc-link" data-astro-cid-xvrfupwn>2. 彻底清理环境（最推荐的快速修复）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-再次尝试安装" class="toc-link" data-astro-cid-xvrfupwn>3. 再次尝试安装</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对你的-ubuntu-系统可能的特殊检查" class="toc-link" data-astro-cid-xvrfupwn>针对你的 Ubuntu 系统可能的特殊检查</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-3-好了我现在在新服务器上已经安装好了要检查一下这个是不是server节" class="toc-link" data-astro-cid-xvrfupwn>排查场景 3. 好了，我现在在新服务器上已经安装好了，要检查一下这个是不是server节…</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-核心状态检查" class="toc-link" data-astro-cid-xvrfupwn>1. 核心状态检查</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-验证参数是否生效" class="toc-link" data-astro-cid-xvrfupwn>2. 验证参数是否生效</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-查看组件运行情况" class="toc-link" data-astro-cid-xvrfupwn>3. 查看组件运行情况</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-关键获取老服务器加入的通行证" class="toc-link" data-astro-cid-xvrfupwn>4. 关键：获取老服务器加入的“通行证”</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#下一步串联老服务器-10001" class="toc-link" data-astro-cid-xvrfupwn>下一步：串联老服务器 (10.0.0.1)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#一个关于-revieu-项目的架构小技巧" class="toc-link" data-astro-cid-xvrfupwn>一个关于 RevieU 项目的架构小技巧</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-14-kubectl-get-pods--a-rootcc-111658" class="toc-link" data-astro-cid-xvrfupwn>主题 14. kubectl get pods -A root@cc 11:16:58</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#现在的状态解读" class="toc-link" data-astro-cid-xvrfupwn>现在的状态解读：</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#关键下一步串联老服务器-10001" class="toc-link" data-astro-cid-xvrfupwn>关键下一步：串联老服务器 (10.0.0.1)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#串联成功后会发生什么" class="toc-link" data-astro-cid-xvrfupwn>串联成功后会发生什么。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#给你的-revieu-项目留个后手" class="toc-link" data-astro-cid-xvrfupwn>给你的 RevieU 项目留个“后手”</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-4-但是老服务器上有一个k3s直接安装行吗" class="toc-link" data-astro-cid-xvrfupwn>排查场景 4. 但是老服务器上有一个k3s，直接安装行吗</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-安全卸载老服务器的-k3s" class="toc-link" data-astro-cid-xvrfupwn>1. 安全卸载老服务器的 k3s</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-清理残留保险起见" class="toc-link" data-astro-cid-xvrfupwn>2. 清理残留（保险起见）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-正式加入新集群-作为-agent" class="toc-link" data-astro-cid-xvrfupwn>3. 正式加入新集群 (作为 Agent)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-验证串联结果" class="toc-link" data-astro-cid-xvrfupwn>4. 验证“串联”结果</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对你老服务器的防爆建议" class="toc-link" data-astro-cid-xvrfupwn>针对你老服务器的“防爆”建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-15-kubectl-get-nodes-rootcc-111816" class="toc-link" data-astro-cid-xvrfupwn>主题 15. kubectl get nodes root@cc 11:18:16</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-现状分析" class="toc-link" data-astro-cid-xvrfupwn>1. 现状分析</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-核心操作给老服务器打补丁" class="toc-link" data-astro-cid-xvrfupwn>2. 核心操作：给老服务器打“补丁”</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-如何在-revieu-项目中使用这个架构" class="toc-link" data-astro-cid-xvrfupwn>3. 如何在 RevieU 项目中使用这个架构。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-验证内网连通性" class="toc-link" data-astro-cid-xvrfupwn>4. 验证内网连通性</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-16-怎么让两个node负载均衡不能老服务器啥也不干啊" class="toc-link" data-astro-cid-xvrfupwn>主题 16. 怎么让两个node负载均衡，不能老服务器啥也不干啊</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#针对-revieu-的最佳实践" class="toc-link" data-astro-cid-xvrfupwn>针对 RevieU 的最佳实践：</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#接下来我们可以做什么" class="toc-link" data-astro-cid-xvrfupwn>接下来我们可以做什么。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-5-这个怎么做" class="toc-link" data-astro-cid-xvrfupwn>排查场景 5. 这个怎么做</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-准备工作确保老服务器可以被调度" class="toc-link" data-astro-cid-xvrfupwn>1. 准备工作：确保老服务器可以被调度</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-编写-deployment-yaml" class="toc-link" data-astro-cid-xvrfupwn>2. 编写 Deployment YAML</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-创建-service-实现流量分发" class="toc-link" data-astro-cid-xvrfupwn>3. 创建 Service 实现流量分发</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#4-为什么这样做能实现动态平衡" class="toc-link" data-astro-cid-xvrfupwn>4. 为什么这样做能实现“动态平衡”。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#进阶建议利用污点容忍-toleration" class="toc-link" data-astro-cid-xvrfupwn>进阶建议：利用污点容忍 (Toleration)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#排查场景-6-我在老服务器上怎么不行" class="toc-link" data-astro-cid-xvrfupwn>排查场景 6. 我在老服务器上，怎么不行</a></li><li class="toc-item depth-2" data-astro-cid-xvrfupwn><a href="#解决方案" class="toc-link" data-astro-cid-xvrfupwn>解决方案</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案一在大脑新服务器上执行最标准" class="toc-link" data-astro-cid-xvrfupwn>方案一：在“大脑”（新服务器）上执行（最标准）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案二把指挥权共享给老服务器如果你想在老服务器操作" class="toc-link" data-astro-cid-xvrfupwn>方案二：把“指挥权”共享给老服务器（如果你想在老服务器操作）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案三在你的笔记本电脑上操作最优雅" class="toc-link" data-astro-cid-xvrfupwn>方案三：在你的笔记本电脑上操作（最优雅）</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#为什么会出现-localhost8080" class="toc-link" data-astro-cid-xvrfupwn>为什么会出现 localhost:8080。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-17-现在这个role还是none啊" class="toc-link" data-astro-cid-xvrfupwn>主题 17. 现在这个role还是none啊</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#1-手动添加-worker-角色-可选" class="toc-link" data-astro-cid-xvrfupwn>1. 手动添加 Worker 角色 (可选)</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#2-为什么显示-none-并不影响负载均衡" class="toc-link" data-astro-cid-xvrfupwn>2. 为什么显示 &lt;none&gt; 并不影响负载均衡。</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#3-接下来建议的操作实现真正的负载均衡" class="toc-link" data-astro-cid-xvrfupwn>3. 接下来建议的操作：实现真正的负载均衡</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#给你的-revieu-项目架构师的小建议" class="toc-link" data-astro-cid-xvrfupwn>给你的 RevieU 项目架构师的小建议</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#主题-18-那我的dns记录应该是把域名解析到两个服务器还是只解析server那台" class="toc-link" data-astro-cid-xvrfupwn>主题 18. 那我的dns记录应该是把域名解析到两个服务器，还是只解析server那台</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案-1只解析到新服务器-10004--最推荐最省心" class="toc-link" data-astro-cid-xvrfupwn>方案 1：只解析到新服务器 (10.0.0.4) —— 最推荐，最省心</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案-2两台都解析轮询-dns-高可用方案" class="toc-link" data-astro-cid-xvrfupwn>方案 2：两台都解析（轮询 DNS）—— 高可用方案</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#方案-3使用-cloudflare-cdn--架构师的选择" class="toc-link" data-astro-cid-xvrfupwn>方案 3：使用 Cloudflare (CDN) —— 架构师的选择</a></li><li class="toc-item depth-3" data-astro-cid-xvrfupwn><a href="#我的建议" class="toc-link" data-astro-cid-xvrfupwn>我的建议</a></li><li class="toc-item depth-2" data-astro-cid-xvrfupwn><a href="#实施与验证清单" class="toc-link" data-astro-cid-xvrfupwn>实施与验证清单</a></li></ul></nav><script type="module">const i=new IntersectionObserver(e=>{e.forEach(t=>{const c=t.target.getAttribute("id"),r=document.querySelector(`.toc-link[href="#${c}"]`);t.isIntersecting&&(document.querySelectorAll(".toc-link.active").forEach(o=>{o.classList.remove("active")}),r?.classList.add("active"))})},{rootMargin:"-80px 0px -80% 0px",threshold:0});document.querySelectorAll("article h2[id], article h3[id]").forEach(e=>{i.observe(e)});</script> </div> </aside> <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo>  <header class="article-header" data-astro-cid-bvzihdzo> <h1 class="article-title" data-astro-cid-bvzihdzo>K3s 双节点集群部署指南</h1> <div class="article-meta" data-astro-cid-bvzihdzo>  <time datetime="2026-02-12T00:00:00.000Z"> Feb 11, 2026 </time> <span class="meta-divider" data-astro-cid-bvzihdzo></span> <span class="reading-time" data-astro-cid-rafwukvt> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" width="16" height="16" data-astro-cid-rafwukvt> <path stroke-linecap="round" stroke-linejoin="round" d="M12 6v6h4.5m4.5 0a9 9 0 1 1-18 0 9 9 0 0 1 18 0Z" data-astro-cid-rafwukvt></path> </svg> 60 min read
</span>   <span class="meta-divider" data-astro-cid-bvzihdzo></span> <span class="last-updated" data-astro-cid-bvzihdzo>
Updated  <time datetime="2026-02-13T00:00:00.000Z"> Feb 12, 2026 </time> </span>  </div> </header> <div class="article-content" data-astro-cid-bvzihdzo>  <h2 id="背景与目标">背景与目标</h2>
<p>本文围绕“我准备在两台vps上部署k3s，然后作为两个界点串联起来”展开，整理可直接复用的步骤、原理说明与排查经验。</p>
<h2 id="核心内容">核心内容</h2>
<h3 id="主题-1-我准备在两台vps上部署k3s然后作为两个界点串联起来">主题 1. 我准备在两台vps上部署k3s，然后作为两个界点串联起来</h3>
<p>在两台 VPS 上部署 k3s 并将它们组成一个集群（一个 Server 节点 + 一个 Agent 节点）是一个非常经典且高效的选择。</p>
<p>既然你熟悉 <strong>Arch Linux</strong>、<strong>Neovim</strong> 并且正在做 <strong>RevieU</strong> 项目的架构，这种“轻量级 Kubernetes”方案非常适合你的开发和测试需求。</p>
<p>以下是实现这一目标的快速指南：</p>
<p>1. 准备工作</p>
<p>在开始之前，确保两台 VPS 满足以下条件：</p>
<ul>
<li><strong>网络连通性</strong>：两台机器之间需要互通。如果是跨云厂商，建议使用 <strong>Tailscale</strong> 或 <strong>WireGuard</strong> 建立虚拟内网，或者确保防火墙开放了以下端口：
<ul>
<li><code>6443</code> (API Server)</li>
<li><code>10250</code> (Kubelet Metrics)</li>
<li><code>8472</code> (Flannel VXLAN)</li>
</ul>
</li>
<li><strong>系统环境</strong>：虽然你喜欢 Arch，但如果 VPS 运行的是其他发行版，k3s 也能完美支持。</li>
</ul>
<p>2. 部署第一个节点 (Server/Master)</p>
<p>在第一台 VPS 上执行，这台将作为集群的控制平面。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | sh -</span></span></code></pre>
<p><strong>关键操作：</strong> 部署完成后，你需要获取 <strong>Node Token</strong>，这是第二台机器加入集群的凭证：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo cat /var/lib/rancher/k3s/server/node-token</span></span></code></pre>
<p>同时也记下这台机器的 <strong>公网 IP</strong>（或内网 IP）。</p>
<p>3. 部署第二个节点 (Agent/Worker)</p>
<p>在第二台 VPS 上执行，将其作为工作节点连接到 Server。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://&#x3C;SERVER_IP>:6443 K3S_TOKEN=&#x3C;NODE_TOKEN> sh -</span></span></code></pre>
<blockquote>
<p>请将 <code>&#x3C;SERVER_IP></code> 替换为第一台服务器的 IP，将 <code>&#x3C;NODE_TOKEN></code> 替换为你刚刚获取的字符串。</p>
</blockquote>
<p>4. 架构示意图</p>
<p>在你的场景中：</p>
<ul>
<li><strong>Server 节点</strong>：运行 API Server、Scheduler 和 Etcd（默认使用 SQLite，适合双节点）。</li>
<li><strong>Agent 节点</strong>：只运行 Kubelet 和 Containerd，负责承载具体的 Pod 运行。</li>
</ul>
<p>5. 进阶建议 (针对你的技术栈)</p>
<p>鉴于你目前的背景，这里有几个额外的小贴士：</p>
<ul>
<li><strong>管理集群</strong>：你可以把第一台机器的 <code>/etc/rancher/k3s/k3s.yaml</code> 复制到本地的 <code>~/.kube/config</code>。这样你就可以在本地用 <code>kubectl</code> 甚至是在 <strong>Neovim</strong> 里通过插件直接管理集群。</li>
<li><strong>ArgoCD 接入</strong>：由于你对 GitOps 感兴趣，建议在集群搭建好后立即部署 ArgoCD。这样你的 <strong>RevieU</strong> 项目（Go 后端 + React 前端）就可以通过 <code>git push</code> 自动更新到这个双节点集群中。</li>
<li><strong>数据库处理</strong>：由于你使用的是 PostgreSQL，建议将其运行在 Server 节点并挂载持久化存储，或者使用外部数据库，以防 Agent 节点漂移导致数据管理复杂化。</li>
</ul>
<p><strong>你想让我帮你写一个针对这两台 VPS 的基础 Docker Compose 转 Kubernetes Deployment 的配置模板吗。</strong></p>
<h3 id="主题-2-我已经有一个node在运行了但是我不想让这个节点作为主要节点因为这台">主题 2. 我已经有一个node在运行了，但是我不想让这个节点作为主要节点，因为这台…</h3>
<p>这是一个非常合理的决策。k3s 默认情况下，Server 节点既是“大脑”（控制平面）也会运行“体力活”（用户 Pod）。如果这台旧服务器性能较弱，我们应该通过 <strong>Taints（污点）</strong> 限制它运行任务，或者直接将新性能好的 VPS 设为新的 Server。</p>
<p>针对你现有的情况，我有两种优化方案：</p>
<h3 id="方案一保留旧节点为-server但禁止它跑业务-pod">方案一：保留旧节点为 Server，但禁止它跑业务 Pod</h3>
<p>如果你不想重装，只是想让旧服务器只负责“发号施令”，不占用内存跑具体的业务程序（如你的 RevieU 后端），你可以给它打上“污点”。</p>
<ol>
<li><strong>在旧节点（Server）上执行：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    # 这里的 &#x3C;node-name> 通常是你的主机名</span></span>
<span class="line"><span>    kubectl taint nodes &#x3C;node-name> node-role.kubernetes.io/control-plane:NoSchedule</span></span></code></pre>
<p>这样，除非你特别指定，否则所有的业务 Pod 都会自动调度到那台性能更好的新 Agent 节点上。</p>
<h3 id="方案二性能强的作为-server旧的作为-agent-推荐">方案二：性能强的作为 Server，旧的作为 Agent (推荐)</h3>
<p>对于双节点集群，通常建议将<strong>网络最稳定、性能较均衡</strong>的机器作为 Server。如果你的新 VPS 性能全方位碾压旧的，重新反向部署其实更健康。</p>
<p><strong>步骤建议：</strong></p>
<ol>
<li><strong>备份旧节点数据</strong>（如果有的话）。</li>
<li><strong>在新 VPS 上部署 Server：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    curl -sfL https://get.k3s.io | sh -</span></span></code></pre>
<ol start="3">
<li><strong>在旧 VPS 上重置并加入：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    # 先卸载旧的 server</span></span>
<span class="line"><span>    /usr/local/bin/k3s-uninstall.sh</span></span>
<span class="line"><span>    # 作为 agent 加入新集群</span></span>
<span class="line"><span>    curl -sfL https://get.k3s.io | K3S_URL=https://&#x3C;新VPS_IP>:6443 K3S_TOKEN=&#x3C;新TOKEN> sh -</span></span></code></pre>
<h3 id="针对你的-revieu-项目架构建议">针对你的 RevieU 项目架构建议</h3>
<p>由于你正在负责 <strong>RevieU</strong> 的 Infra，考虑到性能差异，建议你这样分配资源：</p>






























<table><thead><tr><th>资源类型</th><th>建议放置位置</th><th>原因</th></tr></thead><tbody><tr><td><strong>PostgreSQL 数据库</strong></td><td>性能强的新节点</td><td>数据库对 IO 和 CPU 要求高，旧节点容易产生瓶颈。</td></tr><tr><td><strong>Go (Gin) 后端</strong></td><td>性能强的新节点</td><td>保证 API 响应速度。</td></tr><tr><td><strong>Nginx Proxy Manager</strong></td><td>旧节点</td><td>负载均衡和反向代理对资源消耗相对较低。</td></tr><tr><td><strong>ArgoCD / Monitoring</strong></td><td>性能强的新节点</td><td>这些 DevOps 工具其实挺吃内存的。</td></tr></tbody></table>
<h3 id="一个进阶的小技巧">一个进阶的小技巧</h3>
<p>既然你习惯用 <strong>Arch Linux</strong> 和 <strong>Neovim</strong>，我建议你在新节点部署好后，在本地开启一个 <code>k9s</code>（一个非常强大的终端 UI 管理工具）。它能让你像用 Neovim 快捷键一样管理这两台 VPS 上的 Pod，非常符合你的操作习惯。</p>
<p><strong>需要我帮你写一份针对这两台机器性能差异的 <code>nodeSelector</code> 或 <code>Affinity</code> (亲和性) 配置示例吗。这样你可以精准控制哪些服务跑在强力节点上。</strong></p>
<h3 id="主题-3-server和agent有什么区别">主题 3. server和agent有什么区别</h3>
<p>在 Kubernetes（以及 k3s）的世界里，这就像是一个公司的<strong>管理层</strong>和<strong>员工层</strong>的关系。</p>
<p>虽然 k3s 为了轻量化把很多功能打包在一起，但 Server 和 Agent 的职责分工非常明确：</p>
<h3 id="1-server-节点-control-plane">1. Server 节点 (Control Plane)</h3>
<p><strong>身份：</strong> 集群的“大脑”和“指挥中心”。</p>
<ul>
<li><strong>核心任务：</strong> 决策、存储、调度。</li>
<li><strong>关键组件：</strong>
<ul>
<li><strong>API Server：</strong> 所有的命令（比如你输入的 <code>kubectl</code>）都要经过它。</li>
<li><strong>SQLite/Etcd：</strong> 存储整个集群的状态和配置（k3s 默认在单机使用 SQLite，更省资源）。</li>
<li><strong>Scheduler：</strong> 决定你的 Pod（比如 RevieU 的 Go 后端）应该运行在哪台机器上。</li>
</ul>
</li>
<li><strong>资源消耗：</strong> 主要是内存和 IO。即使不跑任何业务，它也会占用一定的资源来维持集群状态。</li>
</ul>
<h3 id="2-agent-节点-worker-node">2. Agent 节点 (Worker Node)</h3>
<p><strong>身份：</strong> 集群的“体力劳动者”。</p>
<ul>
<li><strong>核心任务：</strong> 运行容器、上报状态。</li>
<li><strong>关键组件：</strong>
<ul>
<li><strong>Kubelet：</strong> 听从 Server 的指令，负责管理这台机器上的容器生命周期。</li>
<li><strong>Containerd：</strong> 实际运行容器的引擎。</li>
<li><strong>Kube-proxy：</strong> 处理网络规则，让外界能访问到你的服务。</li>
</ul>
</li>
<li><strong>资源消耗：</strong> 取决于你运行了多少业务。它本身非常轻量。</li>
</ul>
<h3 id="核心对比表">核心对比表</h3>



































<table><thead><tr><th>特性</th><th>Server 节点</th><th>Agent 节点</th></tr></thead><tbody><tr><td><strong>角色</strong></td><td>控制平面 (Master)</td><td>工作节点 (Worker)</td></tr><tr><td><strong>能否运行 Pod</strong></td><td><strong>可以</strong> (默认行为)</td><td><strong>可以</strong> (唯一任务)</td></tr><tr><td><strong>存储数据</strong></td><td>存储集群配置和状态</td><td>不存储集群配置</td></tr><tr><td><strong>如果宕机</strong></td><td>集群无法接受新指令/部署</td><td>仅丢失该节点上的任务，集群正常运行</td></tr><tr><td><strong>安装命令参数</strong></td><td><code>k3s server</code></td><td><code>k3s agent</code></td></tr></tbody></table>
<h3 id="结合你的情况为什么你要在意这个区别">结合你的情况：为什么你要在意这个区别。</h3>
<p>既然你提到<strong>第一台服务器性能不行</strong>，理解它们的区别对你至关重要：</p>
<ol>
<li><strong>控制权分离</strong>：你可以让性能差的旧机器只当 Server。它不跑具体的业务（Pod），只负责管理。</li>
<li><strong>Server 也能当 Agent</strong>：默认情况下，k3s 的 Server 节点会自动启动 Agent 功能。也就是说，如果不做特殊配置，Server 也会跑业务。
<ul>
<li><em>建议：</em> 如果你决定用性能强的新机器当 Server，那它既是管理节点也是主力运行节点。</li>
<li><em>建议：</em> 如果你让旧机器当 Server，一定要用 <code>taint</code>（污点）把它的业务承载能力关掉，否则它可能会因为内存不足把整个集群带崩。</li>
</ul>
</li>
</ol>
<p><strong>既然你要串联这两台机器，需要我为你提供一套“Server 禁用调度”的具体命令，好让旧机器专注于管理工作吗。</strong></p>
<h3 id="主题-4-那我还是应该先在新服务器上安装k3s然后打通连哥哥节点">主题 4. 那我还是应该先在新服务器上安装k3s然后打通连哥哥节点</h3>
<p>是的，这绝对是最理智的策略。</p>
<p>既然你有一台<strong>性能更强</strong>的新服务器，把它作为 <strong>Server (Master)</strong> 节点是长久之计。这样集群的“大脑”运行速度快，且有足够的余量处理后续的 API 请求和调度。</p>
<p>以下是针对你目前情况的<strong>最佳实践步骤</strong>：</p>
<h3 id="第一步清理旧节点如果已安装-k3s">第一步：清理旧节点（如果已安装 k3s）</h3>
<p>如果你之前在那台性能弱的机器上已经装了 k3s 且它是作为 Server 运行的，请先卸载它，清理环境：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>/usr/local/bin/k3s-uninstall.sh</span></span></code></pre>
<h3 id="第二步在新服务器上部署-server">第二步：在新服务器上部署 Server</h3>
<p>在性能强的新服务器上执行，建议加上 <code>--node-taint</code> 参数。这样可以确保它既是 Server，也能跑业务，同时标记其角色：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | sh -</span></span></code></pre>
<p>安装完成后，获取加入集群所需的 Token：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo cat /var/lib/rancher/k3s/server/node-token</span></span></code></pre>
<h3 id="第三步将旧服务器作为-agent-加入">第三步：将旧服务器作为 Agent 加入</h3>
<p>在性能弱的旧服务器上执行。既然它性能不行，我们只让它干点轻活：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://&#x3C;新服务器_IP>:6443 K3S_TOKEN=&#x3C;刚刚获取的TOKEN> sh -</span></span></code></pre>
<h3 id="第四步打通网络核心关键">第四步：打通网络（核心关键）</h3>
<p>因为你是两台 VPS，通常不在同一个内网。为了让它们“串联”起来，你有两个选择：</p>
<ol>
<li><strong>公网直连（简单但不安全）</strong>： 在安装命令中添加 <code>--node-external-ip</code> 参数，并确保防火墙开放了 <code>6443</code> 和 <code>8472</code> (UDP) 端口。</li>
<li><strong>虚拟内网（推荐）</strong>： 使用 <strong>Tailscale</strong>。给两台机器都装上 Tailscale，然后安装 k3s 时指定 Tailscale 分配的内网 IP（例如 <code>100.x.x.x</code>）。这样数据传输会通过加密隧道，且不需要在公网暴露端口。</li>
</ol>
<h3 id="针对你的配置建议">针对你的配置建议</h3>
<p>部署完成后，在新服务器（Server）上查看节点状态：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl get nodes</span></span></code></pre>
<p><strong>为了保护那台弱鸡旧服务器：</strong> 你可以给旧服务器打上一个标签，比如 <code>size=small</code>。以后在部署 <strong>RevieU</strong> 项目的 PostgreSQL 或核心 Go 后端时，在 YAML 里加上 <code>nodeAffinity</code>，强制它们只在新服务器上运行。而一些轻量级的定时任务、监控埋点或静态页面 Nginx，可以放心地交给旧服务器。</p>
<p><strong>由于你之后要部署 Go (Gin) 后端，需要我帮你写一个带有“节点亲和性（Node Affinity）”的 K8s 部署模板吗。这样可以确保后端服务自动避开性能差的那台机器。</strong></p>
<h3 id="主题-5-我的老服务器上的东西我不想删掉啊">主题 5. 我的老服务器上的东西我不想删掉啊</h3>
<p>我完全理解。作为开发者，老服务器上肯定积攒了不少配置、脚本或者是还没迁移的数据，<strong>千万别随便运行卸载脚本</strong>。</p>
<p>如果想保留老服务器上的现有业务（非 K8s 业务），同时又想让它加入新集群，我们得换一种“温和”的策略。</p>
<h3 id="最稳妥的方案双系统并行">最稳妥的方案：双系统并行</h3>
<p>你不需要删除老服务器上的任何东西。你可以让老服务器<strong>既运行它原本的程序，又作为一个 k3s Agent 节点运行</strong>。</p>
<h4 id="1-确认资源余量">1. 确认资源余量</h4>
<p>k3s Agent 本身非常轻量（大约占用 200MB-300MB 内存）。只要你的老服务器还有点剩余内存，这两个身份是可以共存的。</p>
<h4 id="2-操作步骤">2. 操作步骤</h4>
<ul>
<li><strong>在新服务器（强力机）上：</strong> 直接安装 Server。</li>
</ul>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    curl -sfL https://get.k3s.io | sh -</span></span></code></pre>
<p>拿到 Token 后备用。</p>
<ul>
<li><strong>在老服务器（弱机）上：</strong> <strong>不要卸载任何东西</strong>。直接运行 Agent 安装命令：</li>
</ul>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    curl -sfL https://get.k3s.io | K3S_URL=https://&#x3C;新服务器IP>:6443 K3S_TOKEN=&#x3C;TOKEN> sh -</span></span></code></pre>
<p>k3s 会自动识别系统环境。它会作为一个新的进程运行，不会触碰你原有的 Nginx、数据库或其他应用。</p>
<h3 id="️-必须要解决的冲突端口限制">️ 必须要解决的冲突：端口限制</h3>
<p>这是最关键的一点。如果你的老服务器上已经跑了 <strong>Nginx (80/443端口)</strong> 或者 <strong>数据库 (5432端口)</strong>，而 K8s 里的应用也想用这些端口，就会发生冲突。</p>
<p><strong>解决方案：</strong></p>
<ol>
<li><strong>让 K8s 避让</strong>：在部署 <strong>RevieU</strong> 项目时，不要使用 <code>HostPort</code>，而是使用 <code>NodePort</code>（通常是 30000 以上的端口）或者 <code>Ingress</code>。</li>
<li><strong>禁用 k3s 默认流量入口</strong>：k3s 默认会占用 80/443 端口给 Traefik。如果你的老服务器已经占用了这些端口，安装 Agent 时可以加一个参数，防止它去抢占：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    # 在老服务器安装时</span></span>
<span class="line"><span>    curl -sfL https://get.k3s.io | K3S_URL=... K3S_TOKEN=... sh -s - --kubelet-arg="--image-gc-high-threshold=60"</span></span></code></pre>
<p><em>注：其实 Agent 默认不运行控制平面的组件，冲突概率较小，主要是注意容器的网络映射。</em></p>
<h3 id="保护老服务器的生存空间">保护老服务器的“生存空间”</h3>
<p>既然老服务器性能不行，且还要跑原来的程序，我们必须限制 K8s 往它身上派重活：</p>
<ol>
<li><strong>限制资源使用</strong>： 在 K8s 的 YAML 配置文件里，给 Pod 设置 <code>resources.requests</code> 和 <code>limits</code>。</li>
<li><strong>使用标签控制</strong>： 给老节点打个标签 <code>node-role.kubernetes.io/legacy=true</code>。</li>
<li><strong>调度策略</strong>： 在你的 <strong>RevieU</strong> 部署文件中，明确指定 <code>nodeAffinity</code>（节点亲和性），让核心 Go 后端和数据库只跑在新服务器上。</li>
</ol>
<p><strong>你老服务器上原本运行的是哪些服务。（比如 Nginx、Docker 容器还是纯脚本。）我可以帮你看看具体的端口或资源冲突风险。</strong></p>
<h3 id="主题-6-如果在运行服务的node上添加污点pod会自动迁移到新服务器上吗">主题 6. 如果在运行服务的node上添加污点，pod会自动迁移到新服务器上吗</h3>
<p>简单直接的回答是：<strong>不会立即自动迁移</strong>，除非你手动干预或者 Pod 发生了重启。</p>
<p>在 Kubernetes 中，污点（Taints）和容忍度（Tolerations）的逻辑是这样的：</p>
<h3 id="1-现状已经运行的-pod">1. 现状：已经运行的 Pod</h3>
<p>当你给一个正在运行服务的 Node 添加 <code>NoSchedule</code> 污点时，Kubernetes <strong>不会</strong>主动把已经在那儿运行的 Pod 赶走。</p>
<ul>
<li><code>NoSchedule</code>：仅影响<strong>新</strong> Pod 的调度。已经在运行的 Pod 会继续留在那里“养老”。</li>
<li>如果你想让它们立刻滚蛋，你需要使用更狠的污点效果：<code>NoExecute</code>。</li>
</ul>
<h3 id="2-如何让它们自动迁移">2. 如何让它们“自动”迁移。</h3>
<p>你有两个选择：</p>
<h4 id="方案-a手动驱逐推荐最可控">方案 A：手动驱逐（推荐，最可控）</h4>
<p>在你打完污点后，手动删掉那个 Pod（如果你是用 Deployment 部署的）：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl delete pod &#x3C;pod名称></span></span></code></pre>
<p>由于 Deployment 会维持副本数量，它发现 Pod 没了，就会尝试重建。此时，由于老节点有污点，调度器（Scheduler）只能把新 Pod 塞到你的<strong>新服务器</strong>上。</p>
<h4 id="方案-b使用-noexecute-污点有风险">方案 B：使用 <code>NoExecute</code> 污点（有风险）</h4>
<p>如果你添加的是这个效果：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl taint nodes &#x3C;老节点名称> key=value:NoExecute</span></span></code></pre>
<p>那么在该节点上所有没有对应“容忍度”的 Pod 都会被 <strong>立即驱逐</strong>。它们会被系统杀掉，然后在没有污点的新服务器上重启。 <em>注意：这会导致服务出现短暂的秒级中断。</em></p>
<h3 id="3-针对你的实际操作建议">3. 针对你的实际操作建议</h3>
<p>既然你不想删掉老服务器上的东西（非 K8s 服务），但又想让 K8s 的服务都跑在新服务器上，建议按以下步骤操作：</p>
<ol>
<li><strong>给老节点打上“尽量别来”的污点：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    kubectl taint nodes &#x3C;老节点名称> performance=low:NoSchedule</span></span></code></pre>
<ol start="2">
<li><strong>“温和”地重启你的服务：</strong> 如果你的 <strong>RevieU</strong> 项目是用 Deployment 部署的，执行这个命令可以让它平滑地在新服务器上重建：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    kubectl rollout restart deployment &#x3C;你的项目名称></span></span></code></pre>
<h3 id="总结">总结</h3>
<p>打污点是<strong>设关卡</strong>，不是<strong>赶房客</strong>。</p>
<ul>
<li><strong>打完污点 + <code>kubectl delete pod</code></strong> = 迁移到新服务器。</li>
<li><strong>打完污点 + 什么都不做</strong> = 旧 Pod 依然在老服务器，新 Pod 才会去新服务器。</li>
</ul>
<p><strong>你的 RevieU 项目现在是用 YAML 文件部署的吗。我可以帮你看看如何在 YAML 里直接加上“必须运行在新服务器”的硬性限制（NodeAffinity），这样比污点更保险。</strong></p>
<h3 id="主题-7-我用内网10001还需要https吗">主题 7. 我用内网，10.0.0.1,还需要https吗</h3>
<p>在使用内网 IP（如 <code>10.0.0.1</code>）时，<strong>依然必须保留 <code>https://</code> 前缀</strong>。</p>
<p>这并不是因为我们要走公网加密，而是因为 K3S 的控制平面（API Server）默认就是通过 <strong>HTTPS</strong> 协议监听 <code>6443</code> 端口的。即使在内网环境下，节点之间的通信也会进行双向 TLS 认证以确保集群安全。</p>
<h3 id="1-正确的内网安装命令">1. 正确的内网安装命令</h3>
<p>假设你的新服务器（Server）内网 IP 是 <code>10.0.0.1</code>，你在老服务器（Agent）上应该执行：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.1:6443 K3S_TOKEN=&#x3C;NODE_TOKEN> sh -</span></span></code></pre>
<h3 id="2-为什么内网也强制-https">2. 为什么内网也强制 HTTPS。</h3>
<ul>
<li><strong>身份验证</strong>：K8s 集群依赖证书来确认“这个节点确实是我的 Agent”，防止未经授权的机器接入你的内网集群。</li>
<li><strong>通信加密</strong>：即使在 <code>10.x.x.x</code> 段内，K3S 也会加密传输数据，防止内网抓包窃取敏感配置（比如你的数据库密码 Secret）。</li>
</ul>
<h3 id="3-️-关键检查server-端是否监听了内网-ip">3. ️ 关键检查：Server 端是否监听了内网 IP。</h3>
<p>在两台 VPS 串联时，如果你的 Server 节点默认只绑定了公网 IP，Agent 访问 <code>10.0.0.1</code> 可能会被拒绝。</p>
<p><strong>建议：</strong> 在安装新服务器（Server）时，最好显式指定它监听内网 IP，并将其加入 TLS 证书的许可名单中：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 在新服务器（Server）执行</span></span>
<span class="line"><span>curl -sfL https://get.k3s.io | sh -s - server \</span></span>
<span class="line"><span>  --bind-address=10.0.0.1 \</span></span>
<span class="line"><span>  --advertise-address=10.0.0.1 \</span></span>
<span class="line"><span>  --tls-san=10.0.0.1</span></span></code></pre>
<blockquote>
<p><strong>解释：</strong> <code>--tls-san</code>（Subject Alternative Name）非常重要，它告诉 Server：“如果有人通过 <code>10.0.0.1</code> 这个 IP 找你，你的 SSL 证书是有效的，不要报错。”</p>
</blockquote>
<h3 id="4-针对你两台-vps-的网络排烟troubleshooting">4. 针对你两台 VPS 的网络排烟（Troubleshooting）</h3>
<p>既然你用的是内网 IP，安装前请先在<strong>老服务器</strong>上确认能不能“摸到”新服务器的 6443 端口：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>ping 10.0.0.1</span></span>
<span class="line"><span># 或者用 nc 检查端口（假设你装了 openbsd-netcat）</span></span>
<span class="line"><span>nc -zv 10.0.0.1 6443</span></span></code></pre>
<h3 id="一个关于-arch-linux-的小提醒">一个关于 Arch Linux 的小提醒</h3>
<p>由于你习惯使用 <strong>Arch Linux</strong>，如果你的 VPS 也是 Arch，安装 k3s 后记得检查一下 <code>iptables</code>。Arch 默认的 <code>nftables</code> 有时会拦截 k3s 的流量。如果发现节点 <code>NotReady</code>，可以尝试暂时关闭防火墙测试：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo systemctl stop ufw # 或者其他防火墙服务</span></span></code></pre>
<p><strong>你的这两台 VPS 是在同一个云服务商的同一个 VPC（私有网络）下吗。如果不在，这个 <code>10.0.0.1</code> 可能无法直接访问，需要建立隧道。</strong></p>
<h3 id="主题-8-老服务器上必须开放的端口有哪些">主题 8. 老服务器上必须开放的端口有哪些</h3>
<p>既然你不打算删除老服务器上的原有服务，且要让它作为 <strong>Agent (Worker)</strong> 节点运行，那么端口管理就非常关键。</p>
<p>在 K3s 架构中，<strong>Agent 节点主要扮演“受控端”</strong>，它主动连接 Server 节点。因此，它对外开放的端口其实比 Server 少得多。</p>
<h3 id="1-k3s-内部通讯必须开放的端口">1. K3s 内部通讯必须开放的端口</h3>
<p>为了让集群正常工作，老服务器（Agent）必须确保以下端口在防火墙（如 <code>iptables</code>, <code>ufw</code> 或云服务商安全组）中是通畅的：</p>























<table><thead><tr><th>端口</th><th>协议</th><th>角色</th><th>用途</th></tr></thead><tbody><tr><td><strong>10250</strong></td><td>TCP</td><td>Kubelet</td><td><strong>核心：</strong> 允许 Server 查看节点日志、执行命令 (<code>kubectl exec</code>)。</td></tr><tr><td><strong>8472</strong></td><td>UDP</td><td>Flannel VXLAN</td><td><strong>网络：</strong> 节点间 Pod 互相通信的基础（Overlay 网络）。</td></tr></tbody></table>
<blockquote>
<p><strong>注意：</strong> 如果你是在<strong>内网 (10.0.0.x)</strong> 环境下操作，建议直接对新服务器的内网 IP 开放所有流量，这样最省心。</p>
</blockquote>
<h3 id="2-业务访问端口容易冲突的地方">2. 业务访问端口（容易冲突的地方）</h3>
<p>除了 K3s 自身的端口，你还需要考虑你部署的 <strong>RevieU</strong> 项目：</p>
<ul>
<li><strong>Kube-Proxy (随机范围)</strong>：如果你使用 <code>NodePort</code> 暴露服务，默认会占用 <strong>30000-32767</strong> 之间的端口。</li>
<li><strong>Traefik / Ingress (80/443)</strong>：这是最危险的地方。K3s 默认会启动 Traefik 负载均衡。
<ul>
<li><strong>风险</strong>：如果你老服务器原本就跑着 Nginx 占用了 80 端口，K3s Agent 启动 Traefik 时会报错或抢占失败。</li>
<li><strong>对策</strong>：既然老服务器性能差，<strong>建议在安装 Agent 时禁用 Traefik</strong>（或者干脆只在 Server 节点运行 Ingress）。</li>
</ul>
</li>
</ul>
<h3 id="3-如何在老服务器上温和地安装">3. 如何在老服务器上“温和”地安装。</h3>
<p>为了避免端口冲突，我建议你在老服务器（Agent）执行安装命令时，加入一些限制参数：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://&#x3C;新服务器IP>:6443 \</span></span>
<span class="line"><span>  K3S_TOKEN=&#x3C;TOKEN> \</span></span>
<span class="line"><span>  sh -s - agent \</span></span>
<span class="line"><span>  --node-ip=10.0.0.x \</span></span>
<span class="line"><span>  --disable-selinux</span></span></code></pre>
<h3 id="4-总结你的防火墙检查清单">4. 总结：你的防火墙检查清单</h3>
<p>如果你在老服务器上配置防火墙，请按以下规则设置：</p>
<ol>
<li><strong>入站 (Inbound)</strong>:
<ul>
<li>允许来自 <strong>新服务器内网 IP</strong> 的 <code>TCP 10250</code>。</li>
<li>允许来自 <strong>新服务器内网 IP</strong> 的 <code>UDP 8472</code>。</li>
<li>(可选) 允许你原本服务的端口（如 22, 80, 443）。</li>
</ul>
</li>
<li><strong>出站 (Outbound)</strong>:
<ul>
<li>必须允许访问 <strong>新服务器内网 IP</strong> 的 <code>TCP 6443</code> (API Server)。</li>
</ul>
</li>
</ol>
<h3 id="避坑指南">避坑指南</h3>
<p>由于你习惯使用 <strong>Arch Linux</strong>，记得检查一下系统的 <code>cgroup</code> 支持。K3s 需要 cgroup v2 或 v1。如果启动失败，可以在 <code>/boot/cmdline.txt</code> (或者 grub 配置) 中检查是否开启了必要的内核参数。</p>
<p><strong>你老服务器上原本跑的服务，是通过 Docker 容器运行的，还是直接在 Linux 上跑的二进制/脚本。</strong> 如果是 Docker，我们要额外注意 Docker 和 K3s (Containerd) 同时运行时的网桥冲突。</p>
<h3 id="主题-9-这个可以用ufw实现吗还是怎么做">主题 9. 这个可以用ufw实现吗，还是怎么做</h3>
<p>可以使用 <strong>ufw (Uncomplicated Firewall)</strong> 轻松实现。在 Linux（包括你习惯的 Arch Linux 环境）中，ufw 是管理 iptables 规则最直观的方式。</p>
<p>既然你想对新服务器（Server）的内网 IP 开放所有流量，以下是具体的操作命令：</p>
<h3 id="1-使用-ufw-授权内网-ip">1. 使用 ufw 授权内网 IP</h3>
<p>在**老服务器（Agent）**上执行，假设新服务器的内网 IP 是 <code>10.0.0.1</code>：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 允许来自新服务器的所有 TCP/UDP 流量</span></span>
<span class="line"><span>sudo ufw allow from 10.0.0.1</span></span></code></pre>
<p>执行完这一行，新服务器发往老服务器的任何端口（包括 K3s 必须的 <code>10250</code>、<code>8472</code> 等）就全通了。</p>
<h3 id="2-检查并启用-ufw">2. 检查并启用 ufw</h3>
<p>如果你之前没用过 ufw，记得确认它的状态：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 检查状态</span></span>
<span class="line"><span>sudo ufw status</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 如果是 inactive，先确保 ssh 没被关掉，再启动</span></span>
<span class="line"><span>sudo ufw allow ssh</span></span>
<span class="line"><span>sudo ufw enable</span></span></code></pre>
<h3 id="3-更进阶的做法基于网段授权">3. 更进阶的做法：基于网段授权</h3>
<p>如果你的两台 VPS 在同一个私有子网（比如都是 <code>10.0.0.x</code>），你也可以直接允许整个内网网段，这样以后如果你增加第三个、第四个节点，就不需要再单独配置了：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo ufw allow from 10.0.0.0/24</span></span></code></pre>
<h3 id="️-一个必须要提醒你的arch-用户细节">️ 一个必须要提醒你的“Arch 用户”细节</h3>
<p>由于你平时使用 <strong>Arch Linux</strong>，ufw 底层实际上是在操作 <code>iptables</code> 或 <code>nftables</code>。K3s 会为了实现容器网络（Flannel/Cilium）自己往 <code>iptables</code> 里插入很多复杂的转发规则。</p>
<p><strong>这就可能导致一个经典问题：</strong> ufw 的默认策略（比如 <code>DEFAULT_INPUT_POLICY="DROP"</code>）可能会意外拦截 K3s 内部 Pod 之间的流量。</p>
<p><strong>建议：</strong> 如果安装完 K3s 后发现 <code>kubectl get nodes</code> 显示节点是 <code>Ready</code>，但 Pod 之间无法通信（比如 RevieU 的后端连不上数据库），你需要修改 ufw 的转发政策：</p>
<ol>
<li>编辑配置文件：<code>sudo nvim /etc/default/ufw</code> (或者用你常用的 Neovim)</li>
<li>将 <code>DEFAULT_FORWARD_POLICY</code> 从 <code>"DROP"</code> 改为 <code>"ACCEPT"</code>:</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    DEFAULT_FORWARD_POLICY="ACCEPT"</span></span></code></pre>
<ol start="3">
<li>重启防火墙：<code>sudo ufw reload</code></li>
</ol>
<h3 id="总结操作清单">总结操作清单</h3>
<ol>
<li><strong>老服务器</strong>：执行 <code>sudo ufw allow from 10.0.0.1</code>。</li>
<li><strong>新服务器</strong>：执行 <code>sudo ufw allow from 10.0.0.2</code>（假设老服务器是 .2）。</li>
<li><strong>验证</strong>：在新服务器上尝试 <code>telnet 10.0.0.2 10250</code>，通了就说明 K3s 串联没问题了。</li>
</ol>
<p><strong>你的两台 VPS 是直接在云服务商（如腾讯云、阿里云或 AWS）的控制台里有安全组设置，还是只靠系统内部的 ufw 管理。</strong> 如果有云端安全组，那里也要记得放开内网 IP。</p>
<h3 id="主题-10-老服务器ip是10001新服务器是10004">主题 10. 老服务器ip是10.0.0.1,新服务器是10.0.0.4</h3>
<p>既然 IP 已经明确了，那我们直接对号入座。你只需要在两台机器上分别执行几行命令，就能把“护城河”修好，同时给 K3s 留出专门的通道。</p>
<h3 id="1-在老服务器-10001-上操作">1. 在老服务器 (10.0.0.1) 上操作</h3>
<p>这台机器性能较弱且有旧服务，我们要确保它只信任新服务器的指令：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 1. 允许来自新服务器（Server 节点）的所有内网流量</span></span>
<span class="line"><span>sudo ufw allow from 10.0.0.4</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 2. 确保你原有的服务端口依然开放（假设是 80 和 443）</span></span>
<span class="line"><span>sudo ufw allow 80/tcp</span></span>
<span class="line"><span>sudo ufw allow 443/tcp</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 3. 允许 SSH 访问（防止断开连接）</span></span>
<span class="line"><span>sudo ufw allow ssh</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 4. 修改转发策略（Arch Linux/K3s 必须，否则 Pod 无法联网）</span></span>
<span class="line"><span>sudo sed -i 's/DEFAULT_FORWARD_POLICY="DROP"/DEFAULT_FORWARD_POLICY="ACCEPT"/' /etc/default/ufw</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 5. 生效</span></span>
<span class="line"><span>sudo ufw reload</span></span></code></pre>
<h3 id="2-在新服务器-10004-上操作">2. 在新服务器 (10.0.0.4) 上操作</h3>
<p>这台是你的主力机（Server 节点），它需要接收来自老服务器（Agent）的心跳包和数据流：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 1. 允许来自老服务器的所有内网流量</span></span>
<span class="line"><span>sudo ufw allow from 10.0.0.1</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 2. 开放 K3s 的控制平面端口（虽然前面 all from 已经包含了，但建议显式开放）</span></span>
<span class="line"><span>sudo ufw allow 6443/tcp</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 3. 允许 SSH</span></span>
<span class="line"><span>sudo ufw allow ssh</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 4. 修改转发策略</span></span>
<span class="line"><span>sudo sed -i 's/DEFAULT_FORWARD_POLICY="DROP"/DEFAULT_FORWARD_POLICY="ACCEPT"/' /etc/default/ufw</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 5. 生效</span></span>
<span class="line"><span>sudo ufw reload</span></span></code></pre>
<h3 id="3-部署串联命令">3. 部署串联命令</h3>
<p>网络打通后，你的 K3s 部署命令应该是这样的：</p>
<p><strong>在新服务器 (10.0.0.4) 启动 Server：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | sh -s - server \</span></span>
<span class="line"><span>  --node-ip=10.0.0.4 \</span></span>
<span class="line"><span>  --advertise-address=10.0.0.4 \</span></span>
<span class="line"><span>  --tls-san=10.0.0.4</span></span></code></pre>
<p><strong>在老服务器 (10.0.0.1) 加入 Agent：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.4:6443 \</span></span>
<span class="line"><span>  K3S_TOKEN=&#x3C;新服务器的TOKEN> \</span></span>
<span class="line"><span>  sh -s - agent \</span></span>
<span class="line"><span>  --node-ip=10.0.0.1</span></span></code></pre>
<h3 id="针对-revieu-项目的小提示">针对 RevieU 项目的小提示</h3>
<p>由于老服务器（10.0.0.1）上已经有了旧的 Nginx 或服务，可能会占用 <code>80</code> 或 <code>443</code> 端口。</p>
<ul>
<li><strong>如果冲突了</strong>：新服务器部署好的 <strong>Traefik</strong>（K3s 自带的入口控制器）在老服务器上可能会启动失败。</li>
<li><strong>解决办法</strong>：你可以利用 <strong>Arch/Neovim</strong> 的便携性，直接编辑控制台。在部署你的 <strong>RevieU</strong> 前端时，通过 <code>Service</code> 的 <code>nodeSelector</code> 将流量入口（Ingress）强制固定在性能更强、且端口干净的 <strong>10.0.0.4</strong> 上。</li>
</ul>
<p><strong>既然你现在要把这两台串起来，需要我帮你写一段 <code>nodeAffinity</code>（节点亲和性）的 YAML 片段吗。这样你可以确保 RevieU 的后端只会在 10.0.0.4 上启动。</strong></p>
<h3 id="排查场景-1-这是什么意思我是ubuntu">排查场景 1. 这是什么意思，我是ubuntu</h3>
<ul>
<li>场景：这是什么意思，我是ubuntu</li>
<li>处理要点：<br>
虽然你使用的是 <strong>Ubuntu</strong>，但这一步同样<strong>非常重要</strong>。</li>
</ul>
<p>简单来说，这是在告诉防火墙（ufw）：“<strong>允许流量在不同的网卡之间穿梭。</strong>”</p>
<h3 id="为什么要这么做">为什么要这么做。</h3>
<p>在 Kubernetes (k3s) 中，流量的流动方式和普通服务器不一样：</p>
<ol>
<li><strong>外部流量</strong>进入服务器。</li>
<li><strong>ufw</strong> 拦截流量。</li>
<li><strong>K3s 的虚拟网络（Flannel）</strong> 需要把这个流量分发给运行在内部的 <strong>Pod</strong>（容器）。</li>
</ol>
<p>默认情况下，Ubuntu 的 ufw 为了安全，会将这种“转发（Forward）”行为设为 <code>DROP</code>（丢弃）。如果不改为 <code>ACCEPT</code>，就会出现以下情况：</p>
<ul>
<li>你的 <strong>RevieU</strong> 后端 Pod 运行正常，但无法访问外网去下载插件。</li>
<li>外部用户访问你的公网 IP，流量到了服务器后，被 ufw 拦住，进不去 Pod。</li>
</ul>
<h3 id="具体操作步骤">具体操作步骤</h3>
<p>你可以手动检查并修改，不需要死记硬背 <code>sed</code> 命令：</p>
<ol>
<li><strong>打开配置文件</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    sudo nano /etc/default/ufw</span></span></code></pre>
<ol start="2">
<li><strong>找到这一行</strong>（通常在第 19 行左右）：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    DEFAULT_FORWARD_POLICY="DROP"</span></span></code></pre>
<ol start="3">
<li><strong>修改为</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    DEFAULT_FORWARD_POLICY="ACCEPT"</span></span></code></pre>
<ol start="4">
<li><strong>保存并重启 ufw</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    sudo ufw reload</span></span></code></pre>
<h3 id="这样安全吗">这样安全吗。</h3>
<p>别担心，这并不代表你的服务器“全裸”了。</p>
<ul>
<li><code>DEFAULT_INPUT_POLICY</code> 依然是 <code>DROP</code>（依然会拦截未授权的入站请求）。</li>
<li>我们只是开启了 <strong>转发能力</strong>，让 K3s 内部的网络插件（CNI）能够正常搬运数据包。</li>
</ul>
<h3 id="给你的-revieu-项目避个坑">给你的 RevieU 项目避个坑</h3>
<p>因为你在做 <strong>RevieU</strong> 的后端（Go/Gin），如果你在 Ubuntu 上不改这个设置，你可能会发现：<strong>即使容器启动了，<code>curl localhost:8080</code> 也是通的，但从其他机器访问这个端口就是死活不通。</strong> 改掉这个转发策略就能解决这个问题。</p>
<p><strong>你现在准备好开始在新服务器 (10.0.0.4) 上运行安装脚本了吗。我可以帮你盯着 Server 端的启动参数。</strong></p>
<h3 id="主题-11-你这一通操作是把我原来的服务器变成了agent新服务器变成了serv">主题 11. 你这一通操作，是把我原来的服务器变成了agent，新服务器变成了serv…</h3>
<p>没错，完全正确。</p>
<p>简单说，我建议你把集群的**“大脑”<strong>搬到</strong>新服务器（10.0.0.4）<strong>上，而让</strong>老服务器（10.0.0.1）<strong>退居二线，只当一个</strong>“打工人”**。</p>
<p>这样做对你目前的情况有三个核心好处：</p>
<h3 id="1-保护老服务器的现有服务">1. 保护老服务器的现有服务</h3>
<p>正如你说的，老服务器性能不行，而且你不想删掉上面的东西。</p>
<ul>
<li><strong>作为 Server</strong>：它需要运行数据库（etcd/sqlite）、API 服务、调度器，非常吃 CPU 和内存。</li>
<li><strong>作为 Agent</strong>：它只需要运行一个很轻的 <code>kubelet</code> 进程。它会听从新服务器的指挥，你甚至可以设置它“只待命，不干活”，这样它就能安稳地运行你原本的那些旧服务。</li>
</ul>
<h3 id="2-让-revieu-项目跑得更稳">2. 让 RevieU 项目跑得更稳</h3>
<p>你正在做的 <strong>RevieU</strong> 项目（Go + React + PostgreSQL）对响应速度有要求。</p>
<ul>
<li>将新服务器设为 Server，意味着主要的计算压力都在这台性能好的机器上。</li>
<li>即使老服务器因为性能问题卡顿或重启，整个 K8s 集群的“大脑”在新服务器上依然是清醒的，不会导致整个集群崩溃。</li>
</ul>
<h3 id="3-现在的架构长这样">3. 现在的架构长这样</h3>




















<table><thead><tr><th>角色</th><th>服务器 IP</th><th>任务</th></tr></thead><tbody><tr><td><strong>Server (大脑)</strong></td><td><code>10.0.0.4</code> (新)</td><td>决策、存储、管理，同时也运行你的核心业务 Pod。</td></tr><tr><td><strong>Agent (体力)</strong></td><td><code>10.0.0.1</code> (老)</td><td>听令行事。因为性能差，我们主要用它跑一些不重要的辅助插件。</td></tr></tbody></table>
<h3 id="如果你一定要反过来老服务器当-server会发生什么">如果你一定要反过来（老服务器当 Server），会发生什么。</h3>
<p>如果让老服务器（10.0.0.1）当 Server：</p>
<ol>
<li>它会为了管理新服务器而消耗掉仅剩的资源，可能导致你原本在跑的服务崩溃。</li>
<li>当你用 <code>kubectl</code> 下发指令时，API 响应会非常慢。</li>
<li>如果老服务器宕机，新服务器即便性能再强也会因为失去联络而变成“无头苍蝇”。</li>
</ol>
<h3 id="下一步操作建议">下一步操作建议</h3>
<p>如果你同意这个方案，你现在的执行顺序应该是：</p>
<ol>
<li><strong>在新服务器 (10.0.0.4)</strong> 执行 <code>curl</code> 安装 Server。</li>
<li><strong>在老服务器 (10.0.0.1)</strong> 执行 <code>curl</code> 加上 <code>K3S_URL=https://10.0.0.4:6443</code> 加入集群。</li>
</ol>
<p><strong>需要我把这两条最终的“一键安装脚本”连同 Token 占位符整理给你吗。</strong></p>
<h3 id="主题-12-在解释一下这些参数">主题 12. 在解释一下这些参数</h3>
<p>既然你准备在**新服务器（10.0.0.4）**上正式起航，理解这几个参数能帮你省去以后调试网络和证书的大麻烦。</p>
<p>简单来说，这些参数是在给 K3s 的“大脑”划定边界，告诉它：“<strong>只准走内网，别去公网乱跑。</strong>”</p>
<h3 id="参数详细拆解">参数详细拆解</h3>
<h4 id="1---node-ip10004">1. <code>--node-ip=10.0.0.4</code></h4>
<ul>
<li><strong>含义</strong>：指定当前节点的 IP 地址。</li>
<li><strong>为什么要写</strong>：VPS 通常有两个 IP（公网和内网）。如果你不指定，K3s 可能会随机抓到一个 IP（通常是公网 IP）。</li>
<li><strong>作用</strong>：强制 K3s 把所有的容器服务都绑定在内网 IP 上。这样你两台机器之间的流量就不会跑出外网，既安全又省流量费。</li>
</ul>
<h4 id="2---advertise-address10004">2. <code>--advertise-address=10.0.0.4</code></h4>
<ul>
<li><strong>含义</strong>：向集群其他成员“广播”自己的地址。</li>
<li><strong>为什么要写</strong>：这是告诉老服务器（10.0.0.1）：“嘿，如果你想找我（Server）汇报工作，请认准 <code>10.0.0.4</code>。”</li>
<li><strong>作用</strong>：确保老服务器（Agent）知道通过内网通道来连接，而不是绕道公网去访问。</li>
</ul>
<h4 id="3---tls-san10004">3. <code>--tls-san=10.0.0.4</code></h4>
<ul>
<li><strong>含义</strong>：在 SSL/TLS 证书中添加“备用名称（Subject Alternative Name）”。</li>
<li><strong>为什么要写</strong>：K3s 节点之间通信是强制加密的。如果你用 <code>https://10.0.0.4:6443</code> 访问 Server，但证书里只写了公网 IP 或主机名，连接就会因为“证书不匹配”而失败。</li>
<li><strong>作用</strong>：给证书开个“白名单”，允许通过 <code>10.0.0.4</code> 这个内网地址进行加密连接。</li>
</ul>
<h3 id="整体架构逻辑">整体架构逻辑</h3>
<p>通过这一套参数，你构建了一个**“内网优先”**的闭环集群：</p>
<ol>
<li><strong>安全性</strong>：即使公网防火墙没关严，外部也无法通过公网 IP 轻易混入你的集群内部通讯。</li>
<li><strong>稳定性</strong>：内网带宽通常比公网高且延迟低，对于你 <strong>RevieU</strong> 项目中 Go 后端与数据库之间的通讯非常有利。</li>
<li><strong>兼容性</strong>：解决了由于 SSL 证书校验失败导致的“Node NotReady”经典报错。</li>
</ol>
<h3 id="接下来该做什么">接下来该做什么。</h3>
<p>在新服务器运行完这条命令后，你需要做两件事：</p>
<ol>
<li><strong>获取令牌 (Token)</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    sudo cat /var/lib/rancher/k3s/server/node-token</span></span></code></pre>
<ol start="2">
<li><strong>在老服务器 (10.0.0.1) 执行加入命令</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.4:6443 \</span></span>
<span class="line"><span>    K3S_TOKEN=&#x3C;刚刚获取的TOKEN> \</span></span>
<span class="line"><span>    sh -s - agent --node-ip=10.0.0.1</span></span></code></pre>
<p><strong>等你两台都跑起来了，你可以运行 <code>kubectl get nodes</code>。如果看到两个节点都显示 <code>Ready</code>，就说明你的内网串联大功告成了。需要我帮你写一个简单的测试 Pod 来验证两台机器之间的内网网络是否真的通了吗。</strong></p>
<h3 id="主题-13-你设置了nodeip是一个vpn-ip那我使用ingress的时候还">主题 13. 你设置了nodeip是一个vpn ip，那我使用ingress的时候，还…</h3>
<p>这是一个非常敏锐的问题。你担心的是：<strong>如果 K8s 内部通讯都锁死在内网 IP，那外网用户还能通过 Ingress 访问我的网站吗。</strong></p>
<p>结论是：<strong>完全没问题，甚至更安全。</strong></p>
<p>这里的关键在于理解 **K8s 内部通讯（Node IP）**和 **外部流量入口（Ingress/Service）**是两个不同的平面。</p>
<h3 id="1-为什么-ingress-依然正常工作">1. 为什么 Ingress 依然正常工作。</h3>
<p>K3s 默认自带的 Ingress 控制器（Traefik）会自动监听机器上的 <strong>所有网卡</strong>（包括公网 IP 的 80/443 端口）。</p>
<ul>
<li><strong>Node IP (10.0.0.x)</strong>：只负责集群内部的“悄悄话”（比如 Server 告诉 Agent 该起哪个 Pod 了）。</li>
<li><strong>Ingress (Traefik)</strong>：它像一个大门保安，虽然它住在内网环境里，但它依然会去监听这台 VPS 的公网 IP 端口。</li>
</ul>
<h3 id="2-流量的具体路径">2. 流量的具体路径</h3>
<p>当一个洛杉矶的用户访问你 <strong>RevieU</strong> 的域名时，流量走势如下：</p>
<ol>
<li><strong>用户</strong> -> 访问你的<strong>公网 IP</strong> (例如 <code>1.2.3.4:80</code>)。</li>
<li><strong>操作系统</strong> -> 收到流量，发现 <code>80</code> 端口被 Traefik 占用了。</li>
<li><strong>Traefik (Ingress)</strong> -> 接过流量，根据域名（比如 <code>api.revieu.com</code>）查表。</li>
<li><strong>内网转发</strong> -> Traefik 通过你的<strong>内网 IP (10.0.0.4)</strong> 把流量转给对应的 Pod。</li>
</ol>
<p>所以，设置 <code>node-ip</code> 只是给集群内部通讯加了个“防盗门”，并不会把对外的“窗户”（Ingress）关上。</p>
<h3 id="3-需要注意的一个小坑重点">3. 需要注意的一个小坑（重点）</h3>
<p>虽然 Ingress 能通，但你需要确保你的 <strong>ufw</strong> 或 <strong>云服务商安全组</strong> 允许外网访问 <code>80</code> 和 <code>443</code> 端口：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 在两台服务器上都要允许外网访问网页端口</span></span>
<span class="line"><span>sudo ufw allow 80/tcp</span></span>
<span class="line"><span>sudo ufw allow 443/tcp</span></span></code></pre>
<h3 id="4-进阶如何彻底利用你的双节点">4. 进阶：如何彻底利用你的双节点。</h3>
<p>既然你有两台机器，你可以灵活决定让谁对外接客：</p>
<ul>
<li><strong>新服务器 (10.0.0.4)</strong>：性能强，建议把 Ingress 流量主要引向这里。</li>
<li><strong>老服务器 (10.0.0.1)</strong>：如果它原本就跑着一个 Nginx 占用了 80 端口，你可以在老服务器上安装 Agent 时加上 <code>--disable traefik</code>。</li>
</ul>
<p>这样，你的架构就变成了：<strong>新服务器负责处理外网所有的 K8s 流量，而老服务器只作为备用计算资源，不直接暴露 K8s 服务给外网。</strong></p>
<h3 id="验证建议">验证建议</h3>
<p>等你部署好第一个 Ingress 规则后，直接在浏览器访问你的 <strong>公网 IP</strong>。如果你能看到 Traefik 的 404 页面（因为还没配置具体的路径），就说明外部流量已经成功穿透 ufw 进到 K3s 内部了。</p>
<p><strong>你的域名现在解析到哪台服务器的公网 IP 了。还是你打算用负载均衡器（Load Balancer）把流量分发给这两台。</strong></p>
<h3 id="排查场景-2-nc--zv-10001-6443-29s-rootcc-11">排查场景 2. nc -zv 10.0.0.1 6443 29s root@cc 11:…</h3>
<ul>
<li>场景：nc -zv 10.0.0.1 6443 29s root@cc 11:…</li>
<li>处理要点：<br>
哎呀，出师未捷。看到 <code>Job for k3s.service failed</code> 别担心，这在 K3s 部署中很常见，通常是由于<strong>残留配置</strong>或<strong>端口占用</strong>引起的。</li>
</ul>
<p>既然你之前在 <code>10.0.0.1</code> 运行过 <code>nc -zv</code> 并且通了，说明那台机器上可能还跑着旧的 K3s 进程，或者是这台新机器（<code>10.0.0.4</code>）的环境有些冲突。</p>
<p>我们直接进入“排障三板斧”：</p>
<h3 id="1-查看具体的报错原因">1. 查看具体的报错原因</h3>
<p>这是最关键的一步，它会直接告诉你为什么起不来：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>journalctl -xeu k3s.service</span></span></code></pre>
<p><strong>请留意日志里是否有以下关键词：</strong></p>
<ul>
<li><code>address already in use</code>：说明 6443 端口被占了。</li>
<li><code>failed to find memory cgroup v2</code>：说明系统内核参数没开。</li>
<li><code>invalid character in token</code>：说明之前的配置文件干扰了现在的启动。</li>
</ul>
<h3 id="2-彻底清理环境最推荐的快速修复">2. 彻底清理环境（最推荐的快速修复）</h3>
<p>因为日志里显示 <code>Skipping binary downloaded</code> 和 <code>symlink already exists</code>，说明这台机器上之前装过 K3s 或者有残留。为了让你的 <code>--node-ip</code> 参数生效，我们先清空重来：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 1. 运行卸载脚本（别担心，只卸载这台 10.0.0.4 的 k3s）</span></span>
<span class="line"><span>sudo /usr/local/bin/k3s-uninstall.sh</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 2. 检查是否有残留进程并杀掉（有时卸载脚本杀不干净）</span></span>
<span class="line"><span>sudo ps aux | grep k3s | awk '{print $2}' | xargs kill -9</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 3. 删掉残留的配置目录</span></span>
<span class="line"><span>sudo rm -rf /etc/rancher/k3s /var/lib/rancher/k3s</span></span></code></pre>
<h3 id="3-再次尝试安装">3. 再次尝试安装</h3>
<p>清理干净后，再次执行你的安装命令：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | sh -s - server \</span></span>
<span class="line"><span>  --node-ip=10.0.0.4 \</span></span>
<span class="line"><span>  --advertise-address=10.0.0.4 \</span></span>
<span class="line"><span>  --tls-san=10.0.0.4</span></span></code></pre>
<h3 id="针对你的-ubuntu-系统可能的特殊检查">针对你的 Ubuntu 系统可能的特殊检查</h3>
<p>如果你是 <strong>Ubuntu 24.04</strong> 或较新版本，有两个小坑可能会导致 <code>systemd</code> 启动失败：</p>
<ol>
<li><strong>防火墙冲突</strong>：即使你设置了 <code>ufw</code>，如果 <code>iptables</code> 版本太老也可能出问题。</li>
<li><strong>Cgroup 设置</strong>： 执行：<code>cat /proc/cgroups</code> 如果 <code>memory</code> 这一行最后面是 <code>0</code>，说明内存限制没开，K3s 会崩。 <em>解决方法</em>：修改 <code>/etc/default/grub</code>，在 <code>GRUB_CMDLINE_LINUX_DEFAULT</code> 里加入 <code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code>，然后 <code>sudo update-grub</code> 并重启。</li>
</ol>
<p><strong>你可以把 <code>journalctl -xeu k3s.service</code> 输出的最后 20 行贴给我看看吗。我一眼就能看出是哪个组件在闹脾气。</strong></p>
<h3 id="排查场景-3-好了我现在在新服务器上已经安装好了要检查一下这个是不是server节">排查场景 3. 好了，我现在在新服务器上已经安装好了，要检查一下这个是不是server节…</h3>
<ul>
<li>场景：好了，我现在在新服务器上已经安装好了，要检查一下这个是不是server节…</li>
<li>处理要点：</li>
</ul>
<p>作为开发者，我们要用事实说话。你可以通过以下几个步骤进行验证：</p>
<h3 id="1-核心状态检查">1. 核心状态检查</h3>
<p>运行最常用的命令来看看节点状态：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl get nodes</span></span></code></pre>
<p><strong>看什么：</strong></p>
<ul>
<li><strong>NAME</strong>：应该是你的主机名。</li>
<li><strong>STATUS</strong>：必须是 <strong>Ready</strong>。</li>
<li><strong>ROLES</strong>：必须包含 <strong>control-plane,master</strong>。</li>
</ul>
<h3 id="2-验证参数是否生效">2. 验证参数是否生效</h3>
<p>我们需要确认 K3s 真的听话，绑定到了你的内网 IP：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl get nodes -o wide</span></span></code></pre>
<p><strong>检查重点：</strong></p>
<ul>
<li><strong>INTERNAL-IP</strong>：必须显示为 <strong>10.0.0.4</strong>。如果这里显示的是你的公网 IP，说明之前的参数没生效，后续串联老服务器会出问题。</li>
</ul>
<h3 id="3-查看组件运行情况">3. 查看组件运行情况</h3>
<p>Server 节点会比 Agent 节点多跑很多东西（如 API Server, Scheduler 等）。你可以看看这些核心组件是否健康：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl get pods -n kube-system</span></span></code></pre>
<p>你会看到 <code>traefik</code>、<code>coredns</code>、<code>local-path-provisioner</code> 等 Pod 正在运行。</p>
<h3 id="4-关键获取老服务器加入的通行证">4. 关键：获取老服务器加入的“通行证”</h3>
<p>既然确认了这台是 Server，现在就需要拿到那个 <strong>Token</strong>，好去老服务器上执行串联。</p>
<p>执行命令：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo cat /var/lib/rancher/k3s/server/node-token</span></span></code></pre>
<p><strong>请把这串很长的字符复制下来。</strong></p>
<h3 id="下一步串联老服务器-10001">下一步：串联老服务器 (10.0.0.1)</h3>
<p>现在请切换到你的<strong>老服务器</strong>，执行以下命令。注意我为你加上了 <code>--node-ip</code> 以确保它也走内网：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.4:6443 \</span></span>
<span class="line"><span>  K3S_TOKEN=&#x3C;你刚刚复制的TOKEN> \</span></span>
<span class="line"><span>  sh -s - agent --node-ip=10.0.0.1</span></span></code></pre>
<h3 id="一个关于-revieu-项目的架构小技巧">一个关于 RevieU 项目的架构小技巧</h3>
<p>等你执行完老服务器的加入命令，回到新服务器再跑一次 <code>kubectl get nodes</code>。当你看到两台机器都显示 <code>Ready</code> 时，你的分布式集群就搭好了。</p>
<p><strong>你想让我帮你写一个简单的测试 YAML 吗。我们可以部署一个带页面的 Nginx，看看它会随机落在哪台服务器上，以及你是否能通过内网和外网分别访问它。</strong></p>
<h3 id="主题-14-kubectl-get-pods--a-rootcc-111658">主题 14. kubectl get pods -A root@cc 11:16:58</h3>
<p>漂亮。<code>INTERNAL-IP</code> 显示为 <strong>10.0.0.4</strong>，而且状态是 <strong>Ready</strong>。这意味着你的 Server 节点已经完全按照预期配置好了，所有的内网指令和证书都已经绑定到了私有网络。</p>
<p>顺便提一句，你的主机名 <code>cc.weijun.online</code> 看起来很专业，这也确认了你的域名指向没问题。</p>
<h3 id="现在的状态解读">现在的状态解读：</h3>
<ul>
<li><strong>Traefik (0/1 ContainerCreating)</strong>：这是正常的。因为是第一次启动，它正在下载镜像。几分钟后它就会变成 <code>Running</code>。</li>
<li><strong>Helm-install</strong>：显示 <code>Completed</code> 是正确的，这些只是负责安装插件的临时任务。</li>
<li><strong>Internal IP</strong>：由于你指定了 <code>10.0.0.4</code>，这个集群的内部通讯现在就像在“自家屋里说话”一样安全。</li>
</ul>
<h3 id="关键下一步串联老服务器-10001">关键下一步：串联老服务器 (10.0.0.1)</h3>
<p>现在，请登录到你的<strong>老服务器</strong>。我们将它作为 Agent 节点加入这个“大脑”。</p>
<h4 id="1-先在新服务器上拿入场券-token">1. 先在新服务器上拿“入场券” (Token)</h4>
<p>在新服务器（10.0.0.4）上运行：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo cat /var/lib/rancher/k3s/server/node-token</span></span></code></pre>
<p>复制这串 Token。</p>
<h4 id="2-在老服务器10001上执行加入">2. 在老服务器（10.0.0.1）上执行加入</h4>
<p>直接粘贴下面的命令（记得替换 Token）：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.4:6443 \</span></span>
<span class="line"><span>  K3S_TOKEN=&#x3C;你刚刚复制的TOKEN> \</span></span>
<span class="line"><span>  sh -s - agent --node-ip=10.0.0.1</span></span></code></pre>
<h3 id="串联成功后会发生什么">串联成功后会发生什么。</h3>
<p>当你执行完老服务器的命令后，回到新服务器执行 <code>kubectl get nodes</code>，你会看到：</p>























<table><thead><tr><th>NAME</th><th>STATUS</th><th>ROLES</th><th>INTERNAL-IP</th></tr></thead><tbody><tr><td><strong>cc.weijun.online</strong></td><td>Ready</td><td>control-plane</td><td>10.0.0.4</td></tr><tr><td><strong>(老服务器名)</strong></td><td>Ready</td><td>&#x3C;none></td><td>10.0.0.1</td></tr></tbody></table>
<h3 id="给你的-revieu-项目留个后手">给你的 RevieU 项目留个“后手”</h3>
<p>由于你之前提到<strong>不想删掉老服务器上的东西</strong>，等这个节点加入后，我们可以给它打上一个标签，防止 K8s 乱派活：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 在新服务器上给老节点打标签 (假设名字叫 old-server)</span></span>
<span class="line"><span>kubectl label node &#x3C;老服务器名> hardware-type=low-spec</span></span></code></pre>
<p>这样，以后我们在部署 <strong>RevieU</strong> 的 PostgreSQL 数据库时，就可以在 YAML 里写明：“<strong>绝对不准跑在 hardware-type=low-spec 的节点上</strong>”，从而彻底保护你的老服务器不被撑爆。</p>
<p><strong>老服务器那边执行顺利吗。如果也报了 <code>systemctl</code> 相关的错，记得先跑一遍卸载脚本清理一下旧残留。</strong></p>
<h3 id="排查场景-4-但是老服务器上有一个k3s直接安装行吗">排查场景 4. 但是老服务器上有一个k3s，直接安装行吗</h3>
<ul>
<li>场景：但是老服务器上有一个k3s，直接安装行吗</li>
<li>处理要点：<br>
<strong>绝对不行。</strong> 如果老服务器上已经跑着一个旧的 k3s（哪怕它现在是停止状态），直接运行安装命令极大概率会失败，或者导致两个 k3s 的配置混在一起，最后让你抓狂。</li>
</ul>
<p>由于老服务器现在的身份是 <strong>Server</strong>（它自己也是个“大脑”），如果不卸载，它会拒绝服从新服务器（10.0.0.4）的指挥。</p>
<h3 id="1-安全卸载老服务器的-k3s">1. 安全卸载老服务器的 k3s</h3>
<p>别担心，运行卸载脚本<strong>只会删除 k3s 相关的程序和配置</strong>，不会动你原本在服务器上跑的其他非 K8s 业务（比如你的脚本、Docker 容器等）。</p>
<p>在<strong>老服务器 (10.0.0.1)</strong> 上执行：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 卸载旧的 server</span></span>
<span class="line"><span>sudo /usr/local/bin/k3s-uninstall.sh</span></span></code></pre>
<h3 id="2-清理残留保险起见">2. 清理残留（保险起见）</h3>
<p>有时候旧的证书或 Token 会留在文件夹里，导致新集群加不进去，建议手动清一下：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>sudo rm -rf /etc/rancher/k3s /var/lib/rancher/k3s</span></span></code></pre>
<h3 id="3-正式加入新集群-作为-agent">3. 正式加入新集群 (作为 Agent)</h3>
<p>清理干净后，你可以放心地把这台老服务器作为“员工”挂载到新服务器下了。</p>
<p>在<strong>老服务器 (10.0.0.1)</strong> 执行：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>curl -sfL https://get.k3s.io | K3S_URL=https://10.0.0.4:6443 \</span></span>
<span class="line"><span>  K3S_TOKEN=&#x3C;你从新服务器拿到的TOKEN> \</span></span>
<span class="line"><span>  sh -s - agent --node-ip=10.0.0.1</span></span></code></pre>
<h3 id="4-验证串联结果">4. 验证“串联”结果</h3>
<p>回到你的<strong>新服务器 (10.0.0.4)</strong>，用你熟悉的 <code>kubectl</code> 看看：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl get nodes -o wide</span></span></code></pre>
<p><strong>成功的标志：</strong> 你会看到两条记录，一条是 <code>10.0.0.4</code> (Control-plane)，另一条是 <code>10.0.0.1</code> (none/worker)。</p>
<h3 id="针对你老服务器的防爆建议">针对你老服务器的“防爆”建议</h3>
<p>既然这台机器性能不行，串联成功后，记得在<strong>新服务器</strong>上给它打个“补丁”，防止 K8s 往它身上压太重的活：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 给老节点打个标签，标记为“弱机器”</span></span>
<span class="line"><span>kubectl label node &#x3C;老服务器的主机名> capacity=low</span></span></code></pre>
<p>这样在你以后部署 <strong>RevieU</strong> 的 Go 后端时，我们可以通过配置，让核心服务<strong>优先</strong>跑在新服务器上，老服务器只负责跑一些轻量级的日志收集或静态页面。</p>
<p><strong>老服务器卸载过程中有遇到报错吗。如果没有，现在就可以执行加入命令了。</strong></p>
<h3 id="主题-15-kubectl-get-nodes-rootcc-111816">主题 15. kubectl get nodes root@cc 11:18:16</h3>
<p>太棒了。看到那行 <code>racknerd-7829f7f Ready</code> 出现，说明你的<strong>跨服务器 K3s 集群正式组建成功</strong>。</p>
<p>这种“一主一从”的分布式架构现在已经跑在你的内网（10.0.0.4 和 10.0.0.1）上了。</p>
<h3 id="1-现状分析">1. 现状分析</h3>
<ul>
<li><strong>cc.weijun.online (10.0.0.4)</strong>: 你的 <strong>Server</strong>。它负责掌管全局，同时也是主力计算节点。</li>
<li><strong>racknerd-7829f7f (10.0.0.1)</strong>: 你的 <strong>Agent</strong>。它现在是你的“扩展包”，虽然性能弱，但它已经整装待命，随时可以分担压力。</li>
</ul>
<h3 id="2-核心操作给老服务器打补丁">2. 核心操作：给老服务器打“补丁”</h3>
<p>既然你反复强调老服务器性能不行，我们现在就要在新服务器（10.0.0.4）上对它进行限制。</p>
<h4 id="第一步打标签-label">第一步：打标签 (Label)</h4>
<p>为了方便在部署 <strong>RevieU</strong> 时进行筛选，先给它贴个标签：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl label node racknerd-7829f7f performance=low capacity=small</span></span></code></pre>
<h4 id="第二步打污点-taint--最关键的一步">第二步：打污点 (Taint) —— <strong>最关键的一步</strong></h4>
<p>如果你希望 K8s <strong>默认不要</strong>往老服务器上派活（除非你手动指定），执行这个：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl taint nodes racknerd-7829f7f performance=low:NoSchedule</span></span></code></pre>
<p><strong>这样做的效果</strong>：普通的 Pod（比如你随手运行的测试服务）会自动避开老服务器，只跑在你的新服务器上，从而保护老服务器的 CPU/内存不被占满。</p>
<h3 id="3-如何在-revieu-项目中使用这个架构">3. 如何在 RevieU 项目中使用这个架构。</h3>
<p>由于你习惯用 Neovim 编写配置，当你部署 <strong>RevieU</strong> 的 Go 后端时，你可以在 YAML 中利用这种“串联”关系。</p>
<p>比如，你想让 PostgreSQL 数据库<strong>只</strong>跑在新服务器上，可以这样写：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>spec:</span></span>
<span class="line"><span>  template:</span></span>
<span class="line"><span>    spec:</span></span>
<span class="line"><span>      nodeSelector:</span></span>
<span class="line"><span>        node-role.kubernetes.io/control-plane: "true" # 强制跑在 Server 节点（新服务器）</span></span></code></pre>
<h3 id="4-验证内网连通性">4. 验证内网连通性</h3>
<p>你可以跑一个简单的命令，看看两个节点之间能不能“对话”：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl run net-test --image=busybox -it --rm --restart=Never -- ping -c 4 10.0.0.1</span></span></code></pre>
<p>如果能 ping 通，说明你的 <strong>ufw</strong> 转发策略配置得非常完美。</p>
<p><strong>现在集群搭好了，你想让我帮你写一个完整的 Docker-Compose 转 K8s Deployment 的 YAML 模板吗。我们可以针对你的 RevieU 项目，把后端、前端和数据库的调度逻辑一次性写好。</strong></p>
<h3 id="主题-16-怎么让两个node负载均衡不能老服务器啥也不干啊">主题 16. 怎么让两个node负载均衡，不能老服务器啥也不干啊</h3>
<p>这正是分布式集群最有趣的地方：<strong>物尽其用</strong>。</p>
<p>既然你不希望老服务器（Racknerd）“吃闲饭”，但又怕它被压死，我们可以采取**“差异化负载均衡”<strong>策略。不要简单地平分压力，而是让它们</strong>各司其职**。</p>
<p>1. 方案一：让老服务器跑“轻量级辅助” (推荐)</p>
<p>你可以把一些对 CPU/内存消耗极低，但又必须存在的服务“赶”到老服务器上。</p>
<ul>
<li><strong>适合放在老服务器的服务</strong>：
<ul>
<li><strong>Nginx 反向代理/静态页面</strong>（只负责转发流量，不涉及业务逻辑）。</li>
<li><strong>监控 Agent (Prometheus Exporter)</strong>：收集性能数据。</li>
<li><strong>定时任务 (CronJobs)</strong>：比如每天凌晨同步一次数据库备份。</li>
<li><strong>Redis 缓存</strong>（如果你只做简单的 Key-Value 存储，内存占用很低）。</li>
</ul>
</li>
</ul>
<p><strong>操作方法：</strong> 在你的 Deployment YAML 中使用 <code>nodeSelector</code> 指定老服务器：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>spec:</span></span>
<span class="line"><span>  nodeSelector:</span></span>
<span class="line"><span>    performance: "low"  # 匹配我们之前打的标签</span></span></code></pre>
<p>2. 方案二：利用水平伸缩 (HPA) 实现动态平衡</p>
<p>你可以让 <strong>RevieU</strong> 的 Go 后端跑两个副本。</p>
<ul>
<li><strong>副本 1</strong>：跑在新服务器（主力）。</li>
<li><strong>副本 2</strong>：跑在老服务器（备用）。</li>
</ul>
<p>通过 K8s 的 <strong>Service</strong>，流量会自动在两个节点之间进行负载均衡。</p>
<p>3. 方案三：精准限制 (Resource Quotas)</p>
<p>如果你想让老服务器也跑一些业务，但又要给它原本的非 K8s 服务留出“求生空间”，你<strong>必须</strong>在 K8s 里设置资源限额。</p>
<p><strong>这是保护你老服务器不崩溃的唯一方法：</strong></p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>spec:</span></span>
<span class="line"><span>  containers:</span></span>
<span class="line"><span>  - name: revieu-backend</span></span>
<span class="line"><span>    resources:</span></span>
<span class="line"><span>      requests:</span></span>
<span class="line"><span>        memory: "64Mi"</span></span>
<span class="line"><span>        cpu: "100m"</span></span>
<span class="line"><span>      limits:</span></span>
<span class="line"><span>        memory: "128Mi" # 限制它最多只能吃这么多内存</span></span>
<span class="line"><span>        cpu: "200m"</span></span></code></pre>
<p>4. 移除“污点”，允许调度</p>
<p>如果你之前按照我的建议打了 <code>NoSchedule</code> 污点，老服务器是<strong>永远不会</strong>有活干的。想要它干活，需要先撤销那个命令：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 移除我们之前打的污点</span></span>
<span class="line"><span>kubectl taint nodes racknerd-7829f7f performance=low:NoSchedule-</span></span></code></pre>
<p><em>(注意末尾那个减号 <code>-</code>，代表删除该污点)</em></p>
<h3 id="针对-revieu-的最佳实践">针对 RevieU 的最佳实践：</h3>
<p>由于你正在开发 <strong>RevieU</strong>，我建议你这样分配：</p>

























<table><thead><tr><th>服务名称</th><th>建议节点</th><th>理由</th></tr></thead><tbody><tr><td><strong>PostgreSQL</strong></td><td>新服务器 (10.0.0.4)</td><td>数据库是重灾区，绝对不能放在性能差的机器上。</td></tr><tr><td><strong>Go 后端 (API)</strong></td><td><strong>两台都跑</strong></td><td>负载均衡，即使一台挂了，API 依然可用。</td></tr><tr><td><strong>React 前端静态文件</strong></td><td>老服务器 (10.0.0.1)</td><td>纯静态资源分发，对老服务器压力极小。</td></tr></tbody></table>
<h3 id="接下来我们可以做什么">接下来我们可以做什么。</h3>
<p>我们可以给老服务器设置一个 <strong>“软性偏好” (Preferred Scheduling)</strong>。 <strong>你想让我帮你写一个完整的 <code>Deployment</code> 脚本吗。</strong> 它可以实现：</p>
<ol>
<li>优先把 Pod 放在新服务器。</li>
<li>当新服务器压力大时，自动分流一部分给老服务器。</li>
<li>同时严格限制老服务器的资源占用，不影响你原本在那上面的旧程序。</li>
</ol>
<h3 id="排查场景-5-这个怎么做">排查场景 5. 这个怎么做</h3>
<ul>
<li>场景：这个怎么做</li>
<li>处理要点：<br>
要实现这个方案，我们需要利用 Kubernetes 的 <strong>Pod 反亲和性 (Pod Anti-Affinity)</strong>。</li>
</ul>
<p>它的逻辑是：告诉 K8s “不要把同一个服务的多个副本放在同一个地方”。这样当你设置副本数为 2 时，它就会被迫在一台机器放一个，从而实现物理层面的负载均衡。</p>
<p>以下是针对你 <strong>RevieU</strong> 项目的完整配置思路：</p>
<h3 id="1-准备工作确保老服务器可以被调度">1. 准备工作：确保老服务器可以被调度</h3>
<p>如果你之前给老服务器打过“污点”（Taint），请先确保它被移除，否则副本 2 永远无法在老服务器上启动：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span># 移除之前的 NoSchedule 污点</span></span>
<span class="line"><span>kubectl taint nodes racknerd-7829f7f performance=low:NoSchedule-</span></span></code></pre>
<h3 id="2-编写-deployment-yaml">2. 编写 Deployment YAML</h3>
<p>在你的 Go 后端配置中加入 <code>affinity</code> 项。你可以用你熟悉的 <strong>Neovim</strong> 创建一个 <code>revieu-backend.yaml</code>：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>apiVersion: apps/v1</span></span>
<span class="line"><span>kind: Deployment</span></span>
<span class="line"><span>metadata:</span></span>
<span class="line"><span>  name: revieu-backend</span></span>
<span class="line"><span>spec:</span></span>
<span class="line"><span>  replicas: 2  # 开启两个副本</span></span>
<span class="line"><span>  selector:</span></span>
<span class="line"><span>    matchLabels:</span></span>
<span class="line"><span>      app: revieu-api</span></span>
<span class="line"><span>  template:</span></span>
<span class="line"><span>    metadata:</span></span>
<span class="line"><span>      labels:</span></span>
<span class="line"><span>        app: revieu-api</span></span>
<span class="line"><span>    spec:</span></span>
<span class="line"><span>      # --- 核心配置：Pod 反亲和性 ---</span></span>
<span class="line"><span>      affinity:</span></span>
<span class="line"><span>        podAntiAffinity:</span></span>
<span class="line"><span>          requiredDuringSchedulingIgnoredDuringExecution:</span></span>
<span class="line"><span>          - labelSelector:</span></span>
<span class="line"><span>              matchExpressions:</span></span>
<span class="line"><span>              - key: app</span></span>
<span class="line"><span>                operator: In</span></span>
<span class="line"><span>                values:</span></span>
<span class="line"><span>                - revieu-api</span></span>
<span class="line"><span>            topologyKey: "kubernetes.io/hostname" </span></span>
<span class="line"><span>      # ----------------------------</span></span>
<span class="line"><span>      containers:</span></span>
<span class="line"><span>      - name: gin-server</span></span>
<span class="line"><span>        image: your-repo/revieu-backend:latest</span></span>
<span class="line"><span>        ports:</span></span>
<span class="line"><span>        - containerPort: 8080</span></span>
<span class="line"><span>        resources:</span></span>
<span class="line"><span>          # 记得给老服务器留活路，限制资源占用</span></span>
<span class="line"><span>          limits:</span></span>
<span class="line"><span>            cpu: "200m"</span></span>
<span class="line"><span>            memory: "256Mi"</span></span>
<span class="line"><span>          requests:</span></span>
<span class="line"><span>            cpu: "100m"</span></span>
<span class="line"><span>            memory: "128Mi"</span></span></code></pre>
<h3 id="3-创建-service-实现流量分发">3. 创建 Service 实现流量分发</h3>
<p>有了两个 Pod 之后，你需要一个“统一入口”来自动把请求分给这两台机器。K8s 的 <strong>Service</strong> 会自动处理这部分负载均衡（通常通过 <code>iptables</code> 或 <code>IPVS</code>）。</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>apiVersion: v1</span></span>
<span class="line"><span>kind: Service</span></span>
<span class="line"><span>metadata:</span></span>
<span class="line"><span>  name: revieu-service</span></span>
<span class="line"><span>spec:</span></span>
<span class="line"><span>  selector:</span></span>
<span class="line"><span>    app: revieu-api</span></span>
<span class="line"><span>  ports:</span></span>
<span class="line"><span>    - protocol: TCP</span></span>
<span class="line"><span>      port: 80        # 集群内部访问端口</span></span>
<span class="line"><span>      targetPort: 8080 # Go 程序监听端口</span></span>
<span class="line"><span>  type: ClusterIP     # 或者 NodePort/LoadBalancer</span></span></code></pre>
<h3 id="4-为什么这样做能实现动态平衡">4. 为什么这样做能实现“动态平衡”。</h3>
<ul>
<li><strong>流量分摊</strong>：当用户请求到达时，Service 会以轮询（Round Robin）的方式，一次把请求发给新服务器上的 Pod，下一次发给老服务器上的 Pod。</li>
<li><strong>高可用</strong>：如果你的老服务器因为性能太差卡死了，K8s 会检测到该 Pod 不健康，自动把所有流量引向新服务器，直到老服务器恢复。</li>
<li><strong>资源保护</strong>：通过设置 <code>resources.limits</code>，即使老服务器上的 Pod 遭遇高并发，它也只能吃掉你分配给它的那一点点内存，不会把你老服务器上原本运行的旧服务搞崩溃。</li>
</ul>
<h3 id="进阶建议利用污点容忍-toleration">进阶建议：利用污点容忍 (Toleration)</h3>
<p>如果你还是担心其他不相关的测试 Pod 跑去老服务器，你可以：</p>
<ol>
<li>给老服务器重新打上污点。</li>
<li>只在 <strong>RevieU</strong> 的 YAML 里加入 <code>tolerations</code>。</li>
</ol>
<p>这样，<strong>只有你的 RevieU 后端</strong>能进去老服务器，其他任何 Pod 都会被老服务器拒之门外。</p>
<p><strong>你想让我帮你把这个“白名单容忍度”也加进 YAML 模板里吗。这样能最大程度保护你老服务器的纯净。</strong></p>
<h3 id="排查场景-6-我在老服务器上怎么不行">排查场景 6. 我在老服务器上，怎么不行</h3>
<ul>
<li>场景：我在老服务器上，怎么不行</li>
<li>处理要点：<br>
别担心，这是一个非常经典且容易误解的问题。</li>
</ul>
<p><strong>原因很简单：</strong> 你的老服务器现在是 <strong>Agent（从节点）</strong>。在 Kubernetes 架构中，Agent 节点本身并不运行 API Server，所以它不知道集群的状态。当你直接在老服务器输入 <code>kubectl get nodes</code> 时，它默认会去尝试连接自己本地的 <code>8080</code> 端口，结果当然是 <code>connection refused</code>。</p>
<p>只有 <strong>Server（主节点）</strong> 也就是你的 <code>10.0.0.4</code> 才能直接响应 <code>kubectl</code> 命令。</p>
<h2 id="解决方案">解决方案</h2>
<p>你有三个选择来解决这个问题，按推荐程度排序：</p>
<h3 id="方案一在大脑新服务器上执行最标准">方案一：在“大脑”（新服务器）上执行（最标准）</h3>
<p>所有的集群管理操作（部署 RevieU、查看状态）都应该在新服务器 <code>10.0.0.4</code> 上完成。那里才是集群的控制中心。</p>
<h3 id="方案二把指挥权共享给老服务器如果你想在老服务器操作">方案二：把“指挥权”共享给老服务器（如果你想在老服务器操作）</h3>
<p>如果你非要在老服务器上用 <code>kubectl</code>，你需要把新服务器的“授权文件”考过来：</p>
<ol>
<li><strong>在新服务器 (10.0.0.4) 上运行：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    sudo cat /etc/rancher/k3s/k3s.yaml</span></span></code></pre>
<ol start="2">
<li><strong>在老服务器 (10.0.0.1) 上运行：</strong></li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    mkdir -p ~/.kube</span></span>
<span class="line"><span>    # 将刚才 cat 出来的全部内容粘贴到这个文件里</span></span>
<span class="line"><span>    nvim ~/.kube/config </span></span></code></pre>
<ol start="3">
<li><strong>修改老服务器上的 config 文件：</strong> 打开 <code>~/.kube/config</code>，找到 <code>server: https://127.0.0.1:6443</code> 这一行。 将 <code>127.0.0.1</code> 改成你的新服务器内网 IP：<code>10.0.0.4</code>。</li>
</ol>
<h3 id="方案三在你的笔记本电脑上操作最优雅">方案三：在你的笔记本电脑上操作（最优雅）</h3>
<p>既然你习惯用 <strong>Arch + Neovim</strong>，最好的体验是在你自己的本地电脑（Mac/PC）上管理集群。</p>
<ol>
<li>把新服务器的 <code>/etc/rancher/k3s/k3s.yaml</code> 内容拷贝到你本地电脑的 <code>~/.kube/config</code>。</li>
<li>同样把 IP 从 <code>127.0.0.1</code> 改为新服务器的 <strong>公网 IP</strong>。</li>
<li>本地安装一个 <code>k9s</code>。</li>
</ol>
<p>这样你坐在家里，就能像操作本地 Docker 一样管理这两台 VPS 上的 <strong>RevieU</strong> 项目。</p>
<h3 id="为什么会出现-localhost8080">为什么会出现 localhost:8080。</h3>
<p>这是 <code>kubectl</code> 的默认保底行为。如果它找不到有效的配置文件（kubeconfig），它就会盲目地去撞 <code>localhost:8080</code>。</p>
<p>由于你的老服务器已经成功加入了集群（刚才在新服务器上已经看到它 Ready 了），你<strong>完全不需要</strong>在老服务器上修好这个报错也能运行服务。</p>
<p><strong>你要不要试着在新服务器上跑一下那个副本数为 2 的 YAML。我们可以看看 Pod 是如何自动跨服务器部署的。</strong></p>
<h3 id="主题-17-现在这个role还是none啊">主题 17. 现在这个role还是none啊</h3>
<p>这是完全正常的现象。在 Kubernetes 中，<code>Agent</code> 节点（Worker 节点）默认确实不显示特定的 Role。</p>
<p>只有 <code>Server</code> 节点会因为运行控制平面组件而被标记为 <code>control-plane</code> 或 <code>master</code>。对于老服务器这种纯粹干活的节点，显示 <code>&#x3C;none></code> 恰恰说明它已经成功卸下了“大脑”的重担，成为了一个纯粹的 <strong>Worker Node</strong>。</p>
<p>如果你觉得 <code>&#x3C;none></code> 看着不舒服，或者为了之后 <strong>RevieU</strong> 项目部署时方便识别，我们可以手动给它贴上 <code>worker</code> 的标签。</p>
<h3 id="1-手动添加-worker-角色-可选">1. 手动添加 Worker 角色 (可选)</h3>
<p>虽然这只是个“展示性”的标签，但能让 <code>kubectl get nodes</code> 的输出更整齐。在 <strong>新服务器 (10.0.0.4)</strong> 上运行：</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>kubectl label node racknerd-7829f7f kubernetes.io/role=worker</span></span></code></pre>
<p>执行完后再看 <code>kubectl get nodes</code>，它的 ROLE 就会变成 <code>worker</code> 了。</p>
<h3 id="2-为什么显示-none-并不影响负载均衡">2. 为什么显示 <code>&#x3C;none></code> 并不影响负载均衡。</h3>
<p>Kubernetes 的调度器（Scheduler）并不是根据 <code>ROLE</code> 这一列来派活的，它是根据节点的<strong>状态（Ready）和资源余量</strong>来决定的。</p>
<p>即便显示为 <code>&#x3C;none></code>，只要它是 <code>Ready</code> 状态，当你部署 <strong>RevieU</strong> 的后端副本时，K8s 就会自动发现：“嘿，这里有一个空闲的计算资源”，然后把 Pod 调度过去。</p>
<h3 id="3-接下来建议的操作实现真正的负载均衡">3. 接下来建议的操作：实现真正的负载均衡</h3>
<p>既然你现在的目标是让两个 Node 跑起来，不要让老服务器闲着，你可以尝试部署一个简单的测试：</p>
<ol>
<li><strong>创建一个 2 副本的 Nginx 部署</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    kubectl create deployment net-test --image=nginx --replicas=2</span></span></code></pre>
<ol start="2">
<li><strong>观察 Pod 的分布</strong>：</li>
</ol>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>    kubectl get pods -o wide</span></span></code></pre>
<p>你应该能看到一个 Pod 运行在 <code>cc.weijun.online</code>，另一个 Pod 运行在 <code>racknerd-7829f7f</code>。</p>
<h3 id="给你的-revieu-项目架构师的小建议">给你的 RevieU 项目架构师的小建议</h3>
<p>由于你之后要用 <strong>Go (Gin)</strong> 做后端，我建议你现在的下一步是：<strong>配置本地 Neovim 环境来管理这个集群。</strong></p>
<p>你不想一直开着两个 SSH 窗口切换吧。你可以把新服务器上的 <code>/etc/rancher/k3s/k3s.yaml</code> 考回你本地 Arch Linux 的 <code>~/.kube/config</code>。</p>
<p><strong>需要我教你如何配置本地的 <code>kubectl</code> 和 <code>k9s</code>，让你在本地就能一键给这两台 VPS 下达指令吗。</strong></p>
<h3 id="主题-18-那我的dns记录应该是把域名解析到两个服务器还是只解析server那台">主题 18. 那我的dns记录应该是把域名解析到两个服务器，还是只解析server那台</h3>
<p>这取决于你打算如何使用这两台服务器来承载你的 <strong>RevieU</strong> 项目。</p>
<p>最简单的答案是：<strong>初期建议先解析到新服务器（Server）那台，后期如果你追求高可用，再考虑两台都解析。</strong></p>
<p>以下是三种常见的配置方案，你可以根据目前的开发进度选择：</p>
<h3 id="方案-1只解析到新服务器-10004--最推荐最省心">方案 1：只解析到新服务器 (10.0.0.4) —— <strong>最推荐，最省心</strong></h3>
<p>既然新服务器性能强，且 K3s 默认的 Ingress (Traefik) 就跑在上面，把域名直接指向它最稳。</p>
<ul>
<li><strong>DNS 设置</strong>：<code>A 记录 -> 10.0.0.4 的公网 IP</code>。</li>
<li><strong>流量走向</strong>：用户访问新服务器 -> Traefik 接收 -> Traefik 通过内网发现老服务器上也跑着一个 Go 后端副本 -> 自动把一部分流量转给老服务器。</li>
<li><strong>优点</strong>：配置简单，证书（SSL）管理统一。即便老服务器重启，由于域名没指向它，用户访问不会报错。</li>
</ul>
<h3 id="方案-2两台都解析轮询-dns-高可用方案">方案 2：两台都解析（轮询 DNS）—— <strong>高可用方案</strong></h3>
<p>如果你希望实现“双入口”，即万一新服务器挂了，用户还能从老服务器进去。</p>
<ul>
<li><strong>DNS 设置</strong>：为同一个域名添加两条 A 记录，分别指向两台服务器的公网 IP。</li>
<li><strong>流量走向</strong>：用户的浏览器会随机选择其中一个 IP 进行访问。</li>
<li><strong>前提条件</strong>：
<ol>
<li>你必须确保老服务器上也跑着 Ingress 控制器（K3s 默认会在所有节点跑 Traefik 的副本）。</li>
<li>两台服务器的 <strong>80/443 端口</strong>都必须对公网开放。</li>
</ol>
</li>
<li><strong>缺点</strong>：如果老服务器性能太差导致 Traefik 响应慢，用户会感觉到网页偶尔加载很慢。而且如果某一台宕机，DNS 还没来得及更新，会有 50% 的用户访问失败。</li>
</ul>
<h3 id="方案-3使用-cloudflare-cdn--架构师的选择">方案 3：使用 Cloudflare (CDN) —— <strong>架构师的选择</strong></h3>
<p>作为 USC 的工程学生，我建议你考虑这个更专业的做法，正好你也搜过 Cloudflare。</p>
<ol>
<li>在 Cloudflare 后台设置两条 A 记录（双入口）。</li>
<li>开启 <strong>Cloudflare Proxy (小黄云)</strong>。</li>
<li><strong>优点</strong>：
<ul>
<li><strong>自动避障</strong>：Cloudflare 会检测你的服务器状态。如果老服务器挂了，它会自动只把流量发给新服务器。</li>
<li><strong>安全性</strong>：隐藏你的真实公网 IP，防止 VPS 被攻击。</li>
<li><strong>RevieU 优化</strong>：Cloudflare 的边缘缓存能帮你分担老服务器的压力。</li>
</ul>
</li>
</ol>
<h3 id="我的建议">我的建议</h3>
<p>对于你现在的 <strong>RevieU</strong> 开发阶段：</p>
<p><strong>第一步</strong>：先把域名解析到 <strong>新服务器 (10.0.0.4) 的公网 IP</strong>。 <strong>第二步</strong>：在 K8s 里部署两个副本（按照我们之前说的反亲和性）。 <strong>第三步</strong>：验证一下，当你通过域名访问 API 时，查看 K8s 日志，你会发现新服务器在悄悄地把请求通过内网转发给老服务器。</p>
<p><strong>你现在的域名是在 Cloudflare 托管吗。如果是的话，我们可以直接配置它的 Load Balancer 功能。</strong></p>
<h2 id="实施与验证清单">实施与验证清单</h2>
<ol>
<li>执行前先确认环境、版本和依赖与文中前置条件一致。</li>
<li>涉及系统级变更时，先备份关键配置或创建快照。</li>
<li>完成操作后按验证步骤确认结果，再执行清理或覆盖动作。</li>
</ol>  </div> <footer class="article-footer" data-astro-cid-bvzihdzo> <div class="share-section" data-astro-cid-bvzihdzo> <div class="share-buttons" data-astro-cid-zllr3mxm> <span class="share-label" data-astro-cid-zllr3mxm>Share:</span> <a href="https://twitter.com/intent/tweet?text=K3s%20%E5%8F%8C%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97&url=https%3A%2F%2Fblog.liweijun.com%2Fblog%2Finfra%2Fk3s-%25E5%258F%258C%25E8%258A%2582%25E7%2582%25B9%25E9%259B%2586%25E7%25BE%25A4%25E9%2583%25A8%25E7%25BD%25B2%25E6%258C%2587%25E5%258D%2597%2F" target="_blank" rel="noopener noreferrer" class="share-btn twitter" title="Share on X" data-astro-cid-zllr3mxm> <svg viewBox="0 0 24 24" fill="currentColor" width="18" height="18" data-astro-cid-zllr3mxm> <path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" data-astro-cid-zllr3mxm></path> </svg> <span data-astro-cid-zllr3mxm>Post on X</span> </a> <button class="share-btn copy-link" id="copyLinkBtn" data-url="https://blog.liweijun.com/blog/infra/k3s-%E5%8F%8C%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97/" data-astro-cid-zllr3mxm> <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" width="18" height="18" data-astro-cid-zllr3mxm> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" data-astro-cid-zllr3mxm></path> <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" data-astro-cid-zllr3mxm></path> </svg> <span class="copy-text" data-astro-cid-zllr3mxm>Copy Link</span> </button> </div>  <script type="module">const t=document.getElementById("copyLinkBtn"),e=t?.querySelector(".copy-text");t?.addEventListener("click",async()=>{const o=t.dataset.url||window.location.href;try{await navigator.clipboard.writeText(o),t.classList.add("copied"),e&&(e.textContent="Copied!"),setTimeout(()=>{t.classList.remove("copied"),e&&(e.textContent="Copy Link")},2e3)}catch(c){console.error("Failed to copy:",c)}});</script> </div> <section class="related-posts" data-astro-cid-dpgbfi7r><h3 class="related-title" data-astro-cid-dpgbfi7r><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" width="20" height="20" data-astro-cid-dpgbfi7r><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25" data-astro-cid-dpgbfi7r></path></svg>
Related Posts
</h3><div class="related-grid" data-astro-cid-dpgbfi7r><a href="/blog/infra/kubectl-rollout-命令详解/" class="related-card" data-astro-cid-dpgbfi7r><h4 class="related-card-title" data-astro-cid-dpgbfi7r>kubectl rollout 命令详解</h4><p class="related-card-desc" data-astro-cid-dpgbfi7r>场景： 在 CI/CD 流水线中，确保新版本完全启动后再进行下一步。</p></a><a href="/blog/infra/k8s-vs-k3s-ingress-体验对比/" class="related-card" data-astro-cid-dpgbfi7r><h4 class="related-card-title" data-astro-cid-dpgbfi7r>k8s vs k3s: Ingress 体验对比</h4><p class="related-card-desc" data-astro-cid-dpgbfi7r>原生 k8s： 如果你不在云平台上，这个 Service 会永远卡在 状态，因为 k8s 不知道如何去向你的路由器或公网申请一个 IP。</p></a><a href="/blog/infra/rclone-vs-google-drive-desktop/" class="related-card" data-astro-cid-dpgbfi7r><h4 class="related-card-title" data-astro-cid-dpgbfi7r>Rclone vs. Google Drive Desktop</h4><p class="related-card-desc" data-astro-cid-dpgbfi7r>无缝体验： 安装即用，支持文件版本历史、右键共享、Google 相册备份。</p></a></div></section> <section class="comments-section" id="comments" data-astro-cid-jvxsf75u> <h3 class="comments-title" data-astro-cid-jvxsf75u> <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" width="22" height="22" data-astro-cid-jvxsf75u> <path stroke-linecap="round" stroke-linejoin="round" d="M7.5 8.25h9m-9 3H12m-9.75 1.51c0 1.6 1.123 2.994 2.707 3.227 1.129.166 2.27.293 3.423.379.35.026.67.21.865.501L12 21l2.755-4.133a1.14 1.14 0 0 1 .865-.501 48.172 48.172 0 0 0 3.423-.379c1.584-.233 2.707-1.626 2.707-3.228V6.741c0-1.602-1.123-2.995-2.707-3.228A48.394 48.394 0 0 0 12 3c-2.392 0-4.744.175-7.043.513C3.373 3.746 2.25 5.14 2.25 6.741v6.018Z" data-astro-cid-jvxsf75u></path> </svg>
Comments
</h3> <div class="giscus-wrapper" data-astro-cid-jvxsf75u> <script src="https://giscus.app/client.js" data-repo="LosFurina/LosFurina.github.io" data-repo-id="R_kgDOQ_gmWQ" data-category="General" data-category-id="DIC_kwDOQ_gmWc4C2A9-" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="en" data-loading="lazy" crossorigin="anonymous" async></script> </div> </section>  <script type="module">function s(){const e=document.documentElement.getAttribute("data-theme"),t=document.querySelector("iframe.giscus-frame");if(t){const i=e==="dark"?"dark":"light";t.contentWindow?.postMessage({giscus:{setConfig:{theme:i}}},"https://giscus.app")}}const n=new MutationObserver(e=>{e.forEach(t=>{t.attributeName==="data-theme"&&s()})});n.observe(document.documentElement,{attributes:!0});window.addEventListener("message",e=>{e.origin==="https://giscus.app"&&s()});</script> </footer> </article> </main> <aside class="right-sidebar" data-astro-cid-bvzihdzo> <!-- Future content --> </aside> </div> <footer data-astro-cid-sz7xmlte> <div class="footer-content" data-astro-cid-sz7xmlte> <p class="copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Wayne Li. All rights reserved.
</p> <div class="social-links" data-astro-cid-sz7xmlte> <a href="https://github.com/LosFurina" target="_blank" rel="noopener" title="GitHub" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>GitHub</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="24" height="24" data-astro-cid-sz7xmlte> <path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-sz7xmlte></path> </svg> </a> <a href="https://x.com/MicroNahida" target="_blank" rel="noopener" title="X (Twitter)" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>X (Twitter)</span> <svg viewBox="0 0 24 24" aria-hidden="true" width="24" height="24" data-astro-cid-sz7xmlte> <path fill="currentColor" d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" data-astro-cid-sz7xmlte></path> </svg> </a> <a href="mailto:me@liweijun.com" title="Email" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Email</span> <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" aria-hidden="true" width="24" height="24" data-astro-cid-sz7xmlte> <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z" data-astro-cid-sz7xmlte></path> <polyline points="22,6 12,13 2,6" data-astro-cid-sz7xmlte></polyline> </svg> </a> <a href="/rss.xml" title="RSS Feed" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>RSS Feed</span> <svg viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" width="24" height="24" data-astro-cid-sz7xmlte> <path d="M6.18 15.64a2.18 2.18 0 0 1 2.18 2.18C8.36 19 7.38 20 6.18 20C5 20 4 19 4 17.82a2.18 2.18 0 0 1 2.18-2.18M4 4.44A15.56 15.56 0 0 1 19.56 20h-2.83A12.73 12.73 0 0 0 4 7.27V4.44m0 5.66a9.9 9.9 0 0 1 9.9 9.9h-2.83A7.07 7.07 0 0 0 4 12.93V10.1z" data-astro-cid-sz7xmlte></path> </svg> </a> </div> <p class="footer-note" data-astro-cid-sz7xmlte>
Built with <a href="https://astro.build" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>Astro</a> </p> </div> </footer>  </body></html>